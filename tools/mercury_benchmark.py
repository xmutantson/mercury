#!/usr/bin/env python3
"""
Mercury Modem Benchmark & Stress Test Suite.

Generates VARA-style performance charts (bytes/min vs SNR) and stress-tests
the modem against adversarial noise conditions.

Sub-commands:
  sweep     Fixed-config SNR sweep — measures throughput at each SNR level per config
  stress    Random noise bursts — tests gearshift, BREAK, and recovery under adversarial noise
  adaptive  Gearshift SNR sweep — single session with gearshift, sweeps SNR down (and optionally back up)

Requires: sounddevice, numpy, matplotlib (optional, for chart generation)

Usage:
  python mercury_benchmark.py sweep [options]
  python mercury_benchmark.py stress [options]
  python mercury_benchmark.py adaptive [options]
"""
import subprocess, socket, time, sys, threading, os, argparse, csv, random
from datetime import datetime
import numpy as np

MERCURY_DEFAULT = r"C:\Program Files\Mercury\mercury.exe"
VB_OUT = "CABLE Input (VB-Audio Virtual Cable)"
VB_IN  = "CABLE Output (VB-Audio Virtual Cable)"
RSP_PORT = 7002
CMD_PORT = 7006
SAMPLE_RATE = 48000

CONFIG_NAMES = {
    100: "ROBUST_0", 101: "ROBUST_1", 102: "ROBUST_2",
    **{i: f"CONFIG_{i}" for i in range(17)}
}

CONFIG_NAMES_NB = {
    100: "ROBUST_0_NB", 101: "ROBUST_1_NB", 102: "ROBUST_2_NB",
    **{i: f"CONFIG_{i}_NB" for i in range(17)}
}

# Theoretical max throughput (bps) per config — from Mercury config table
CONFIG_MAX_BPS = {
    100: 14, 101: 22, 102: 87,
    0: 84, 1: 168, 2: 252, 3: 337, 4: 421,
    5: 631, 6: 841, 7: 1051, 8: 1262, 9: 1682,
    10: 2102, 11: 2523, 12: 2943, 13: 3363,
    14: 3924, 15: 4785, 16: 5735,
}

# Narrowband (Nc=10) PHY bitrates — ROBUST measured via BER, OFDM estimated
# OFDM NB: same LDPC K bits but 5x longer frames (Nsymb scaled by Nc ratio)
CONFIG_MAX_BPS_NB = {
    100: 7, 101: 9, 102: 42,   # Measured: 6.82, 9.17, 41.93 bps
    0: 18, 1: 36, 2: 54, 3: 72, 4: 90,
    5: 143, 6: 191, 7: 250, 8: 300, 9: 400,
    10: 500, 11: 600, 12: 700, 13: 800,
    14: 950, 15: 1150, 16: 1400,
}

# Config presets for common benchmark scenarios
CONFIG_PRESETS = {
    'nb-mfsk': 'nb:100,nb:101,nb:102',
    'wb-mfsk': 'wb:100,wb:101,wb:102',
    'nb-ofdm': ','.join(f'nb:{i}' for i in range(17)),
    'wb-ofdm': ','.join(f'wb:{i}' for i in range(17)),
    'all-nb': 'nb:100,nb:101,nb:102,' + ','.join(f'nb:{i}' for i in range(17)),
    'all-wb': 'wb:100,wb:101,wb:102,' + ','.join(f'wb:{i}' for i in range(17)),
}


def parse_config_spec(spec_str, default_nb=False):
    """Parse config specs like 'nb:100,wb:101,0' into list of (config_id, is_nb).

    Supports presets: 'nb-mfsk', 'wb-mfsk', 'nb-ofdm', 'wb-ofdm', 'all-nb', 'all-wb'
    Combine with commas: 'nb-mfsk,wb-mfsk,nb-ofdm'
    Without prefix, inherits default_nb from --narrowband flag.
    """
    parts = spec_str.split(',')
    expanded = []
    for p in parts:
        p = p.strip()
        if p in CONFIG_PRESETS:
            expanded.extend(CONFIG_PRESETS[p].split(','))
        else:
            expanded.append(p)

    specs = []
    for s in expanded:
        s = s.strip()
        if not s:
            continue
        if s.startswith('nb:'):
            specs.append((int(s[3:]), True))
        elif s.startswith('wb:'):
            specs.append((int(s[3:]), False))
        else:
            specs.append((int(s), default_nb))
    return specs


def config_name(config_id, is_nb):
    """Get display name for a config."""
    names = CONFIG_NAMES_NB if is_nb else CONFIG_NAMES
    return names.get(config_id, f'CONFIG_{config_id}')


def config_max_bps(config_id, is_nb):
    """Get theoretical max throughput for a config."""
    table = CONFIG_MAX_BPS_NB if is_nb else CONFIG_MAX_BPS
    return table.get(config_id, 100)


def build_config_extra_args(is_nb, signal_dbfs):
    """Build Mercury CLI args for a specific config's NB/WB mode and cable level."""
    extra = ["-Q", "0"]  # Disable NB auto-negotiation probing (both sides same mode)
    if is_nb:
        extra += ["-M", "nb"]    # Force nb_only mode (stays NB, no WB upgrade)
    else:
        extra += ["-M", "auto"]  # Auto mode: starts NB, negotiates WB upgrade
    native_dbfs = (NoiseInjector.DEFAULT_SIGNAL_DBFS_NB if is_nb
                   else NoiseInjector.DEFAULT_SIGNAL_DBFS)
    atten_db = signal_dbfs - native_dbfs
    extra += ["-T", f"{atten_db:.1f}", "-G", f"{-atten_db:.1f}"]
    return extra


def auto_measure_duration(config_id, is_nb, base_duration, max_duration=600):
    """Scale measurement duration up for slow modes to get enough data."""
    bps = config_max_bps(config_id, is_nb)
    arq_efficiency = 0.4  # conservative ARQ overhead estimate
    expected_bytes_per_s = bps * arq_efficiency / 8
    min_bytes_target = 200  # want at least 200 bytes for reliable measurement
    needed = min_bytes_target / max(expected_bytes_per_s, 0.01)
    return min(max_duration, max(base_duration, needed))


# ============================================================
# Reused infrastructure (from awgn_turboshift_test.py)
# ============================================================

def find_wasapi_cable_output():
    """Find the WASAPI device index for 'CABLE Input' (output direction)."""
    import sounddevice as sd
    for i, d in enumerate(sd.query_devices()):
        if ('CABLE Input' in d['name'] and
            d['max_output_channels'] >= 1 and
            d['hostapi'] == 2):
            return i
    for i, d in enumerate(sd.query_devices()):
        if 'CABLE Input' in d['name'] and d['max_output_channels'] >= 1:
            return i
    raise RuntimeError("Could not find CABLE Input output device")


class NoiseInjector:
    """Plays AWGN noise on VB-Cable via WASAPI shared mode.

    SNR is referenced to a 4 kHz noise bandwidth, matching HF modem convention
    (PACTOR, WINMOR, etc.). This makes the SNR axis directly comparable between
    NB and WB modes and with other HF modem benchmarks.

    White noise at fs=48kHz has PSD = sigma^2 / (fs/2).
    Noise power in 4 kHz ref BW = sigma^2 * REF_BW / (fs/2).
    So: sigma = signal_amp * sqrt(fs / (2 * REF_BW)) * 10^(-SNR/20)
    Correction factor: sqrt(48000 / 8000) = sqrt(6) = 2.449 (+7.78 dB).

    signal_dbfs sets the reference level. Default -4.4 dBFS = 0.6 amplitude,
    matching Mercury's WB OFDM theoretical RMS with peak clipping.
    """

    # Mercury OFDM passband RMS: sqrt(Nc)/sqrt(Nfft) * carrier_amplitude
    # = sqrt(50)/sqrt(256) * sqrt(2) ≈ 0.625 → -4.1 dBFS
    DEFAULT_SIGNAL_DBFS = -4.4  # conservative estimate accounting for peak clip
    # Narrowband (Nc=10): sqrt(10)/sqrt(256) * sqrt(2) ≈ 0.279 → -11.1 dBFS
    DEFAULT_SIGNAL_DBFS_NB = -11.1
    # Reference noise bandwidth (Hz) — HF standard, matches PACTOR benchmarks
    NOISE_REF_BW = 4000

    def __init__(self, device_index, snr_db=30, sample_rate=48000, signal_dbfs=None):
        self.device_index = device_index
        self.sample_rate = sample_rate
        self.playing = False
        self._stream = None
        self._rng = np.random.default_rng()
        self.signal_dbfs = signal_dbfs if signal_dbfs is not None else self.DEFAULT_SIGNAL_DBFS
        self.signal_amplitude = 10 ** (self.signal_dbfs / 20.0)
        # Bandwidth correction: scale noise so SNR is measured in NOISE_REF_BW
        # sqrt(fs / (2 * ref_bw)) — ratio of Nyquist BW to reference BW
        self.bw_correction = np.sqrt(self.sample_rate / (2.0 * self.NOISE_REF_BW))
        self.set_snr(snr_db)

    def set_snr(self, snr_db):
        self.snr_db = snr_db
        # Noise amplitude: referenced to 4 kHz bandwidth
        # sigma = signal_amp * bw_correction * 10^(-SNR/20)
        self.noise_amplitude = self.signal_amplitude * self.bw_correction * 10 ** (-snr_db / 20.0)

    def _callback(self, outdata, frames, time_info, status):
        if self.playing:
            noise = self._rng.normal(0, self.noise_amplitude, (frames, 1)).astype(np.float32)
            outdata[:] = noise
        else:
            outdata[:] = 0

    def start(self):
        import sounddevice as sd
        self._stream = sd.OutputStream(
            device=self.device_index, samplerate=self.sample_rate,
            channels=1, dtype='float32', callback=self._callback, blocksize=1024,
        )
        self._stream.start()
        print(f"[NOISE] Signal ref: {self.signal_dbfs:.1f} dBFS (amplitude={self.signal_amplitude:.4f})")
        print(f"[NOISE] Ref bandwidth: {self.NOISE_REF_BW} Hz, correction: {self.bw_correction:.3f} ({20*np.log10(self.bw_correction):.1f} dB)")
        print(f"[NOISE] SNR={self.snr_db:.1f}dB (in {self.NOISE_REF_BW}Hz) -> noise amplitude={self.noise_amplitude:.4f}")

    def noise_on(self):
        self.playing = True

    def noise_off(self):
        self.playing = False

    def stop(self):
        self.playing = False
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None


class MultiChannelNoiseInjector:
    """Plays independent AWGN noise on each channel of a multi-channel WASAPI device.

    Each channel has its own SNR level, controlled thread-safely via set_channel_snr().
    Uses the same 4 kHz reference bandwidth as NoiseInjector.
    """

    NOISE_REF_BW = 4000  # Hz — HF standard

    def __init__(self, device_index, num_channels, sample_rate=48000, signal_dbfs=None):
        self.device_index = device_index
        self.num_channels = num_channels
        self.sample_rate = sample_rate
        self._lock = threading.Lock()
        self._rng = np.random.default_rng()
        self._stream = None
        if signal_dbfs is None:
            signal_dbfs = NoiseInjector.DEFAULT_SIGNAL_DBFS
        self.signal_dbfs = signal_dbfs
        self.signal_amplitude = 10 ** (signal_dbfs / 20.0)
        self.bw_correction = np.sqrt(sample_rate / (2.0 * self.NOISE_REF_BW))
        # Per-channel noise amplitudes (0 = silent)
        self.noise_amplitudes = np.zeros(num_channels)

    def set_channel_snr(self, channel, snr_db):
        """Thread-safe: set noise level for one channel."""
        amp = self.signal_amplitude * self.bw_correction * 10 ** (-snr_db / 20.0)
        with self._lock:
            self.noise_amplitudes[channel] = amp

    def silence_channel(self, channel):
        """Mute noise on a channel."""
        with self._lock:
            self.noise_amplitudes[channel] = 0.0

    def silence_all(self):
        """Mute all channels."""
        with self._lock:
            self.noise_amplitudes[:] = 0.0

    def _callback(self, outdata, frames, time_info, status):
        with self._lock:
            amps = self.noise_amplitudes.copy()
        for ch in range(self.num_channels):
            if amps[ch] > 0:
                outdata[:, ch] = self._rng.normal(0, amps[ch], frames).astype(np.float32)
            else:
                outdata[:, ch] = 0.0

    def start(self):
        import sounddevice as sd
        self._stream = sd.OutputStream(
            device=self.device_index, samplerate=self.sample_rate,
            channels=self.num_channels, dtype='float32',
            callback=self._callback, blocksize=1024)
        self._stream.start()
        print(f"[NOISE-MC] Started {self.num_channels}ch noise on device {self.device_index}, "
              f"ref={self.signal_dbfs:.1f}dBFS, bw_corr={self.bw_correction:.3f}")

    def stop(self):
        self.silence_all()
        if self._stream:
            self._stream.stop()
            self._stream.close()
            self._stream = None


def collect_output(proc, lines_list, stop_event):
    for line in iter(proc.stdout.readline, b''):
        if stop_event.is_set():
            break
        text = line.decode('utf-8', errors='replace').rstrip()
        lines_list.append(text)


def count_pattern(lines, pattern, start_idx=0):
    return sum(1 for l in lines[start_idx:] if pattern in l)


def parse_stat_delta(lines, stat_name, start_idx):
    """Get the change in a cumulative stat counter during [start_idx:].

    Mercury prints stats like 'stats.nNAcked_data= 5' every second.
    Returns (last_value - value_at_start_idx), i.e. the delta during the window.
    """
    def _last_value(line_list):
        for line in reversed(line_list):
            if stat_name in line:
                try:
                    return int(line.split('=')[-1].strip())
                except (ValueError, IndexError):
                    continue
        return 0
    val_before = _last_value(lines[:start_idx]) if start_idx > 0 else 0
    val_after = _last_value(lines)
    return max(0, val_after - val_before)


def tcp_connect_retry(port, timeout=5, retries=10, delay=1):
    """Connect to TCP port with retries. Returns socket."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.settimeout(timeout)
    for attempt in range(retries):
        try:
            sock.connect(("127.0.0.1", port))
            return sock
        except (ConnectionRefusedError, socket.timeout):
            if attempt == retries - 1:
                raise
            time.sleep(delay)
            sock.close()
            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            sock.settimeout(timeout)
    return sock


def tcp_send_commands(port, commands):
    """Connect to TCP control port and send commands. Returns socket."""
    sock = tcp_connect_retry(port)
    for cmd in commands:
        sock.sendall(cmd.encode())
        time.sleep(0.3)
        try:
            sock.settimeout(0.5)
            sock.recv(4096)
        except socket.timeout:
            pass
    return sock


def build_extra_args(args):
    """Build Mercury CLI extra args from benchmark flags (narrowband, tx-attenuate).
    Used by stress and adaptive sub-commands. Sweep uses build_config_extra_args() instead."""
    extra = ["-Q", "0"]  # Disable NB auto-negotiation probing
    narrowband = getattr(args, 'narrowband', False)

    if narrowband:
        extra += ["-M", "nb"]    # Force nb_only mode (stays NB, no WB upgrade)
    else:
        extra += ["-M", "auto"]  # Auto mode: starts NB, negotiates WB upgrade

    # Always apply TX/RX gain when cable level differs from native
    native_dbfs = (NoiseInjector.DEFAULT_SIGNAL_DBFS_NB if narrowband
                   else NoiseInjector.DEFAULT_SIGNAL_DBFS)
    atten_db = args.signal_dbfs - native_dbfs  # negative = attenuate
    if abs(atten_db) > 0.5:  # only if meaningful difference
        extra += ["-T", f"{atten_db:.1f}", "-G", f"{-atten_db:.1f}"]
        print(f"[GAIN] TX {atten_db:.1f} dB (cable at {args.signal_dbfs:.1f} dBFS), "
              f"RX +{-atten_db:.1f} dB (restores to ~{native_dbfs:.1f} dBFS)")

    return extra


def get_config_names(narrowband=False):
    """Get config name dict for current mode."""
    return CONFIG_NAMES_NB if narrowband else CONFIG_NAMES


def get_config_max_bps(narrowband=False):
    """Get max BPS dict for current mode."""
    return CONFIG_MAX_BPS_NB if narrowband else CONFIG_MAX_BPS


# ============================================================
# MercurySession — manages a commander+responder pair
# ============================================================

class MercurySession:
    """Manages a commander+responder modem pair lifecycle."""

    def __init__(self, config, gearshift=False, mercury_path=MERCURY_DEFAULT,
                 extra_args=None, rsp_port=None, cmd_port=None,
                 vb_in=None, vb_out=None, audio_channel=None):
        self.config = config
        self.gearshift = gearshift
        self.mercury_path = mercury_path
        self.robust = config >= 100
        self.extra_args = extra_args or []
        self.rsp_port = rsp_port if rsp_port is not None else RSP_PORT
        self.cmd_port = cmd_port if cmd_port is not None else CMD_PORT
        self.vb_in = vb_in if vb_in is not None else VB_IN
        self.vb_out = vb_out if vb_out is not None else VB_OUT
        self.audio_channel = audio_channel  # None = use default, 0-15 = specific channel

        self.procs = []
        self.sockets = []
        self.cmd_lines = []
        self.rsp_lines = []
        self.stop_event = threading.Event()
        self._tx_thread = None
        self._rx_thread = None
        self._rx_bytes = 0
        self._rx_lock = threading.Lock()
        self._rx_events = []  # (timestamp, delta_bytes, total_bytes)

    def _is_nb_mode(self):
        """Check if this session is configured for narrowband mode."""
        if '-N' in self.extra_args:
            return True
        for i, arg in enumerate(self.extra_args):
            if arg == '-M' and i + 1 < len(self.extra_args) and self.extra_args[i + 1] == 'nb':
                return True
        return False

    def start(self, timeout=30, kill_all=True):
        """Launch both processes, connect TCP, start TX thread. Returns True on success.
        If kill_all=False, skip the global taskkill (for parallel mode)."""
        if kill_all:
            os.system("taskkill /F /IM mercury.exe 2>nul >nul")
            time.sleep(1)

        robust_flag = ["-R"] if self.robust else []
        gear_flag = ["-g"] if self.gearshift else []
        channel_flag = ["-A", str(self.audio_channel)] if self.audio_channel is not None else []

        # Start responder
        rsp_cmd = [
            self.mercury_path, "-m", "ARQ", "-s", str(self.config),
            *robust_flag, *gear_flag, *channel_flag, *self.extra_args,
            "-p", str(self.rsp_port), "-i", self.vb_in, "-o", self.vb_out, "-x", "wasapi", "-n"
        ]
        rsp = subprocess.Popen(rsp_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        self.procs.append(rsp)
        threading.Thread(target=collect_output, args=(rsp, self.rsp_lines, self.stop_event), daemon=True).start()
        time.sleep(4)

        # Start commander
        cmd_cmd = [
            self.mercury_path, "-m", "ARQ", "-s", str(self.config),
            *robust_flag, *gear_flag, *channel_flag, *self.extra_args,
            "-p", str(self.cmd_port), "-i", self.vb_in, "-o", self.vb_out, "-x", "wasapi", "-n"
        ]
        cmd = subprocess.Popen(cmd_cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        self.procs.append(cmd)
        threading.Thread(target=collect_output, args=(cmd, self.cmd_lines, self.stop_event), daemon=True).start()
        time.sleep(3)

        # Setup responder — send BW500 for NB mode before MYCALL
        nb_cmd = ["BW500\r\n"] if self._is_nb_mode() else []
        rsp_ctrl = tcp_send_commands(self.rsp_port, nb_cmd + ["MYCALL TESTB\r\n", "LISTEN ON\r\n"])
        self.sockets.append(rsp_ctrl)
        self._rsp_ctrl = rsp_ctrl
        time.sleep(1)

        # Data ports
        cmd_data = tcp_connect_retry(self.cmd_port + 1)
        self.sockets.append(cmd_data)
        rsp_data = tcp_connect_retry(self.rsp_port + 1)
        self.sockets.append(rsp_data)

        # TX thread (pre-fill FIFO)
        self._tx_sock = cmd_data
        self._tx_thread = threading.Thread(target=self._tx_loop, daemon=True)
        self._tx_thread.start()
        time.sleep(2)

        # RX thread
        self._rx_sock = rsp_data
        self._rx_thread = threading.Thread(target=self._rx_loop, daemon=True)
        self._rx_thread.start()

        # Connect — send BW500 for NB mode before CONNECT
        cmd_nb_cmd = ["BW500\r\n"] if self._is_nb_mode() else []
        cmd_ctrl = tcp_send_commands(self.cmd_port, cmd_nb_cmd + ["CONNECT TESTA TESTB\r\n"])
        self.sockets.append(cmd_ctrl)
        self._cmd_ctrl = cmd_ctrl

        return True

    def _tx_loop(self):
        chunk = bytes(range(256)) * 4
        self._tx_sock.settimeout(30)
        while not self.stop_event.is_set():
            try:
                self._tx_sock.send(chunk)
            except socket.timeout:
                continue
            except (ConnectionError, OSError):
                break
            time.sleep(0.05)

    def _rx_loop(self):
        self._rx_sock.settimeout(2)
        loop_count = 0
        while not self.stop_event.is_set():
            try:
                data = self._rx_sock.recv(4096)
                if not data:
                    print(f"  [RX] Socket closed (empty recv) after {loop_count} iterations", flush=True)
                    break
                with self._rx_lock:
                    self._rx_bytes += len(data)
                    self._rx_events.append((time.time(), len(data), self._rx_bytes))
            except socket.timeout:
                loop_count += 1
                continue
            except (ConnectionError, OSError) as e:
                print(f"  [RX] Socket error: {e} after {loop_count} iterations", flush=True)
                break

    def set_noise_snr(self, snr_db):
        """Set internal AWGN noise level on both cmd and rsp via NOISESNR command."""
        cmd = f"NOISESNR {snr_db:.1f}\r"
        for ctrl in (self._rsp_ctrl, self._cmd_ctrl):
            try:
                ctrl.sendall(cmd.encode())
                ctrl.settimeout(0.5)
                ctrl.recv(256)  # consume OK
            except Exception:
                pass

    def silence_noise(self):
        """Turn off internal noise injection on both sides."""
        cmd = b"NOISESNR OFF\r"
        for ctrl in (self._rsp_ctrl, self._cmd_ctrl):
            try:
                ctrl.sendall(cmd)
                ctrl.settimeout(0.5)
                ctrl.recv(256)
            except Exception:
                pass

    def wait_connected(self, timeout=120):
        """Wait for CONNECTED status on control port.
        Uses a line buffer to handle TCP stream fragmentation — keywords like
        CONNECTED can be split across recv() calls when BUFFER messages flood."""
        self._cmd_ctrl.settimeout(2)
        start = time.time()
        buf = b''
        while time.time() - start < timeout:
            try:
                data = self._cmd_ctrl.recv(4096)
                if data:
                    buf += data
                    # Process complete lines (delimited by \r or \n)
                    while b'\r' in buf or b'\n' in buf:
                        idx = len(buf)
                        for delim in (b'\r', b'\n'):
                            pos = buf.find(delim)
                            if pos >= 0 and pos < idx:
                                idx = pos
                        line = buf[:idx]
                        buf = buf[idx+1:]
                        if b'CONNECTED' in line and b'DISCONNECTED' not in line:
                            return True
                        if b'DISCONNECTED' in line:
                            return False
            except socket.timeout:
                continue
            except ConnectionError:
                return False
            if not self.is_alive():
                return False
        return False

    def wait_turboshift(self, timeout=300):
        """Wait for turboshift reverse complete."""
        start = time.time()
        seen_cmd = 0
        seen_rsp = 0
        while time.time() - start < timeout:
            while seen_cmd < len(self.cmd_lines):
                if "TURBO] REVERSE complete" in self.cmd_lines[seen_cmd]:
                    return True
                seen_cmd += 1
            while seen_rsp < len(self.rsp_lines):
                if "TURBO] REVERSE complete" in self.rsp_lines[seen_rsp]:
                    return True
                seen_rsp += 1
            if not self.is_alive():
                return False
            time.sleep(0.5)
        return False

    def wait_data_batches(self, n, timeout=120):
        """Wait until commander stdout shows nAcked_data >= n.
        This confirms that data frames have been sent and acknowledged."""
        import re
        start = time.time()
        seen = 0
        best = 0
        while time.time() - start < timeout:
            while seen < len(self.cmd_lines):
                m = re.search(r'stats\.nAcked_data=\s*(\d+)', self.cmd_lines[seen])
                if m:
                    val = int(m.group(1))
                    if val > best:
                        best = val
                    if best >= n:
                        return True
                seen += 1
            if not self.is_alive():
                return False
            time.sleep(0.5)
        return False

    def measure_throughput(self, duration_s):
        """Measure RX bytes over a time window. Returns dict with stats."""
        with self._rx_lock:
            start_bytes = self._rx_bytes
            start_events = len(self._rx_events)
        start_time = time.time()

        # Periodic progress check during long measurements
        remaining = duration_s
        while remaining > 0:
            chunk = min(remaining, 30)
            time.sleep(chunk)
            remaining -= chunk
            if remaining > 0:
                with self._rx_lock:
                    cur = self._rx_bytes - start_bytes
                if cur > 0:
                    elapsed = time.time() - start_time
                    sys.stdout.write(f"    [{elapsed:.0f}s] +{cur}B so far\n")
                    sys.stdout.flush()

        end_time = time.time()
        with self._rx_lock:
            end_bytes = self._rx_bytes
            end_events = len(self._rx_events)

        rx = end_bytes - start_bytes
        dt = end_time - start_time
        bps = rx * 8 / dt if dt > 0 else 0
        bpm = rx * 60 / dt if dt > 0 else 0

        return {
            'rx_bytes': rx,
            'duration_s': dt,
            'throughput_bps': bps,
            'bytes_per_min': bpm,
            'events': end_events - start_events,
        }

    def get_rx_bytes(self):
        with self._rx_lock:
            return self._rx_bytes

    def get_stats(self, start_idx=0):
        """Parse modem stdout for event counts since start_idx."""
        # NAcks: parse cumulative stats delta (avoids false matches on "nNAcked"/"nAcked")
        nacks_rsp = (parse_stat_delta(self.rsp_lines, "nNAcked_data", start_idx) +
                     parse_stat_delta(self.rsp_lines, "nNAcked_control", start_idx))
        nacks_cmd = (parse_stat_delta(self.cmd_lines, "nNAcked_data", start_idx) +
                     parse_stat_delta(self.cmd_lines, "nNAcked_control", start_idx))
        return {
            'nacks_rsp': nacks_rsp,
            'nacks_cmd': nacks_cmd,
            'breaks_rsp': count_pattern(self.rsp_lines, "[BREAK]", start_idx),
            'breaks_cmd': count_pattern(self.cmd_lines, "[BREAK]", start_idx),
            'geardown': count_pattern(self.cmd_lines, "LADDER DOWN", start_idx),
            'gearup': count_pattern(self.cmd_lines, "LADDER UP", start_idx),
            'break_drops': count_pattern(self.cmd_lines, "Dropping", start_idx),
            'break_recovery': count_pattern(self.cmd_lines, "BREAK-RECOVERY", start_idx),
            'config_changes': count_pattern(self.rsp_lines, "PHY-REINIT", start_idx),
        }

    def get_current_config(self):
        """Find the most recent config from modem output."""
        for line in reversed(self.rsp_lines):
            if "CFG" in line and "load_configuration" in line:
                return line.strip()
        for line in reversed(self.cmd_lines):
            if "CFG" in line and "load_configuration" in line:
                return line.strip()
        return "unknown"

    def is_alive(self):
        return all(p.poll() is None for p in self.procs)

    def stop(self, kill_all=False):
        """Stop this session's processes. If kill_all=True, also run global taskkill."""
        self.stop_event.set()
        for s in self.sockets:
            try: s.close()
            except: pass
        for p in self.procs:
            try:
                p.terminate()
                p.wait(timeout=3)
            except:
                try: p.kill()
                except: pass
        if kill_all:
            os.system("taskkill /F /IM mercury.exe 2>nul >nul")
        time.sleep(0.5)

    def save_log(self, path):
        """Save full modem output to log file."""
        with open(path, 'w') as f:
            f.write("=== RSP OUTPUT ===\n")
            for line in self.rsp_lines:
                f.write(f"[RSP] {line}\n")
            f.write("\n=== CMD OUTPUT ===\n")
            for line in self.cmd_lines:
                f.write(f"[CMD] {line}\n")


# ============================================================
# VaraSession — connects to user-started VARA HF instances
# ============================================================

class VaraSession:
    """Connects to two running VARA HF instances (commander + responder).

    Unlike MercurySession, this does NOT launch or kill processes.
    The user must start two VARA HF instances manually with different
    TCP ports and audio devices pointed at VB-Cable.

    Signal level is controlled via Windows mixer (not CLI flags).
    """

    def __init__(self, rsp_port=8300, cmd_port=8310, bandwidth='2300'):
        self.rsp_port = rsp_port
        self.cmd_port = cmd_port
        self.bandwidth = bandwidth
        self.sockets = []
        self.stop_event = threading.Event()
        self._tx_thread = None
        self._rx_thread = None
        self._tx_sock = None
        self._rx_sock = None
        self._cmd_ctrl = None
        self._rx_bytes = 0
        self._rx_lock = threading.Lock()
        self._rx_events = []

    def start(self, timeout=30):
        """Connect to both VARA instances and set up the link."""
        # Connect to responder control port
        rsp_ctrl = tcp_connect_retry(self.rsp_port, timeout=timeout)
        self.sockets.append(rsp_ctrl)

        # Send responder setup commands
        for cmd in [f"BW{self.bandwidth}\r", "MYCALL TESTB\r", "LISTEN ON\r"]:
            rsp_ctrl.sendall(cmd.encode())
            time.sleep(0.3)
            try:
                rsp_ctrl.settimeout(0.5)
                rsp_ctrl.recv(4096)
            except socket.timeout:
                pass

        time.sleep(2)

        # Connect to commander control port
        cmd_ctrl = tcp_connect_retry(self.cmd_port, timeout=timeout)
        self.sockets.append(cmd_ctrl)
        self._cmd_ctrl = cmd_ctrl

        # Connect data ports
        cmd_data = tcp_connect_retry(self.cmd_port + 1, timeout=timeout)
        self.sockets.append(cmd_data)
        rsp_data = tcp_connect_retry(self.rsp_port + 1, timeout=timeout)
        self.sockets.append(rsp_data)

        # Start TX thread (pumps data into commander)
        self._tx_sock = cmd_data
        self._tx_thread = threading.Thread(target=self._tx_loop, daemon=True)
        self._tx_thread.start()
        time.sleep(1)

        # Start RX thread (reads data from responder)
        self._rx_sock = rsp_data
        self._rx_thread = threading.Thread(target=self._rx_loop, daemon=True)
        self._rx_thread.start()

        # Send commander connect command
        for cmd in [f"BW{self.bandwidth}\r", "CONNECT TESTA TESTB\r"]:
            cmd_ctrl.sendall(cmd.encode())
            time.sleep(0.3)
            try:
                cmd_ctrl.settimeout(0.5)
                cmd_ctrl.recv(4096)
            except socket.timeout:
                pass

        return True

    def _tx_loop(self):
        chunk = bytes(range(256)) * 4
        self._tx_sock.settimeout(30)
        while not self.stop_event.is_set():
            try:
                self._tx_sock.send(chunk)
            except socket.timeout:
                continue
            except (ConnectionError, OSError):
                break
            time.sleep(0.05)

    def _rx_loop(self):
        self._rx_sock.settimeout(2)
        while not self.stop_event.is_set():
            try:
                data = self._rx_sock.recv(4096)
                if not data:
                    break
                with self._rx_lock:
                    self._rx_bytes += len(data)
                    self._rx_events.append((time.time(), len(data), self._rx_bytes))
            except socket.timeout:
                continue
            except (ConnectionError, OSError):
                break

    def wait_connected(self, timeout=120):
        """Wait for CONNECTED status on commander control port."""
        self._cmd_ctrl.settimeout(2)
        start = time.time()
        buf = b''
        while time.time() - start < timeout:
            try:
                data = self._cmd_ctrl.recv(4096)
                if data:
                    buf += data
                    while b'\r' in buf:
                        idx = buf.find(b'\r')
                        line = buf[:idx]
                        buf = buf[idx+1:]
                        if b'CONNECTED' in line and b'DISCONNECTED' not in line:
                            return True
                        if b'DISCONNECTED' in line:
                            return False
            except socket.timeout:
                continue
            except ConnectionError:
                return False
        return False

    def measure_throughput(self, duration_s):
        """Measure RX bytes over a time window. Returns dict with stats."""
        with self._rx_lock:
            start_bytes = self._rx_bytes
        start_time = time.time()

        remaining = duration_s
        while remaining > 0:
            chunk = min(remaining, 30)
            time.sleep(chunk)
            remaining -= chunk
            if remaining > 0:
                with self._rx_lock:
                    cur = self._rx_bytes - start_bytes
                if cur > 0:
                    elapsed = time.time() - start_time
                    sys.stdout.write(f"    [{elapsed:.0f}s] +{cur}B so far\n")
                    sys.stdout.flush()

        end_time = time.time()
        with self._rx_lock:
            end_bytes = self._rx_bytes

        rx = end_bytes - start_bytes
        dt = end_time - start_time
        bps = rx * 8 / dt if dt > 0 else 0
        bpm = rx * 60 / dt if dt > 0 else 0

        return {
            'rx_bytes': rx,
            'duration_s': dt,
            'throughput_bps': bps,
            'bytes_per_min': bpm,
        }

    def get_rx_bytes(self):
        with self._rx_lock:
            return self._rx_bytes

    def is_alive(self):
        """Check if sockets are still connected."""
        for s in self.sockets:
            try:
                s.getpeername()
            except (OSError, socket.error):
                return False
        return True

    def stop(self):
        """Disconnect and close sockets. Does NOT kill VARA processes."""
        self.stop_event.set()
        # Send DISCONNECT
        if self._cmd_ctrl:
            try:
                self._cmd_ctrl.sendall(b"DISCONNECT\r")
                time.sleep(1)
            except (ConnectionError, OSError):
                pass
        for s in self.sockets:
            try:
                s.close()
            except:
                pass
        time.sleep(0.5)

    def save_log(self, path):
        """Save session summary to log file."""
        with open(path, 'w') as f:
            f.write(f"VARA Session Log\n")
            f.write(f"Responder port: {self.rsp_port}\n")
            f.write(f"Commander port: {self.cmd_port}\n")
            f.write(f"Bandwidth: {self.bandwidth}\n")
            f.write(f"Total RX bytes: {self._rx_bytes}\n")
            f.write(f"RX events: {len(self._rx_events)}\n")


# ============================================================
# Chart generation
# ============================================================

def generate_sweep_chart(csv_path, output_path):
    """Generate VARA-style throughput vs SNR chart from sweep CSV."""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
    except ImportError:
        print("[CHART] matplotlib not available. Install with: pip install matplotlib")
        print(f"[CHART] CSV data saved to {csv_path} -- you can plot it manually.")
        return False

    # Read CSV
    configs = {}
    with open(csv_path, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            cfg = int(row['config'])
            name = row['config_name']
            snr = float(row['snr_db'])
            bpm = float(row['bytes_per_min'])
            if cfg not in configs:
                configs[cfg] = {'name': name, 'snr': [], 'bpm': []}
            configs[cfg]['snr'].append(snr)
            configs[cfg]['bpm'].append(bpm)

    if not configs:
        print("[CHART] No data to plot.")
        return False

    # Color palette
    colors = plt.cm.viridis(np.linspace(0, 1, len(configs)))

    fig, ax = plt.subplots(figsize=(14, 8))

    for idx, (cfg, data) in enumerate(sorted(configs.items())):
        snr = data['snr']
        bpm = data['bpm']
        # Filter out zero-throughput trailing points for cleaner lines
        last_nonzero = -1
        for i, v in enumerate(bpm):
            if v > 0:
                last_nonzero = i
        if last_nonzero < 0:
            continue
        snr_plot = snr[:last_nonzero + 2]  # Include one zero point for waterfall edge
        bpm_plot = bpm[:last_nonzero + 2]

        ax.plot(snr_plot, bpm_plot, 'o-', color=colors[idx], label=data['name'],
                linewidth=2, markersize=4)

        # Theoretical max as dashed line
        is_nb = '_NB' in data['name']
        bps_table = CONFIG_MAX_BPS_NB if is_nb else CONFIG_MAX_BPS
        if cfg in bps_table:
            max_bpm = bps_table[cfg] / 8 * 60  # bps → bytes/min
            ax.axhline(y=max_bpm, color=colors[idx], linestyle='--', alpha=0.3, linewidth=1)

    ax.set_xlabel('SNR (dB in 4 kHz)', fontsize=12)
    ax.set_ylabel('Throughput (bytes/min)', fontsize=12)
    ax.set_title('Mercury Modem — Throughput vs SNR', fontsize=14)
    ax.legend(loc='upper left', fontsize=8, ncol=2)
    ax.grid(True, alpha=0.3)
    ax.set_yscale('log')
    ax.set_xlim(right=max(s for d in configs.values() for s in d['snr']) + 2)

    plt.tight_layout()
    plt.savefig(output_path, dpi=150)
    print(f"[CHART] Saved to {output_path}")
    plt.close()
    return True


def generate_stress_chart(timeline_csv, output_path):
    """Generate stress test timeline chart."""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
    except ImportError:
        print("[CHART] matplotlib not available.")
        return False

    timestamps = []
    rx_bytes = []
    noise_on = []

    with open(timeline_csv, 'r') as f:
        reader = csv.DictReader(f)
        for row in reader:
            timestamps.append(float(row['elapsed_s']))
            rx_bytes.append(int(row['rx_bytes_cumulative']))
            noise_on.append(int(row['noise_on']))

    if not timestamps:
        return False

    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8), sharex=True,
                                     gridspec_kw={'height_ratios': [3, 1]})

    # Throughput (cumulative bytes)
    ax1.plot(timestamps, rx_bytes, 'b-', linewidth=1.5)
    ax1.set_ylabel('Cumulative RX Bytes', fontsize=12)
    ax1.set_title('Mercury Modem — Stress Test Timeline', fontsize=14)
    ax1.grid(True, alpha=0.3)

    # Shade noise periods
    in_noise = False
    noise_start = 0
    for i, n in enumerate(noise_on):
        if n and not in_noise:
            noise_start = timestamps[i]
            in_noise = True
        elif not n and in_noise:
            ax1.axvspan(noise_start, timestamps[i], alpha=0.15, color='red')
            ax2.axvspan(noise_start, timestamps[i], alpha=0.15, color='red')
            in_noise = False
    if in_noise:
        ax1.axvspan(noise_start, timestamps[-1], alpha=0.15, color='red')
        ax2.axvspan(noise_start, timestamps[-1], alpha=0.15, color='red')

    # Noise on/off indicator
    ax2.fill_between(timestamps, noise_on, step='post', alpha=0.5, color='red', label='Noise ON')
    ax2.set_ylabel('Noise', fontsize=12)
    ax2.set_xlabel('Time (s)', fontsize=12)
    ax2.set_yticks([0, 1])
    ax2.set_yticklabels(['OFF', 'ON'])

    plt.tight_layout()
    plt.savefig(output_path, dpi=150)
    print(f"[CHART] Saved to {output_path}")
    plt.close()
    return True


# ============================================================
# Multi-channel parallel sweep infrastructure
# ============================================================

from collections import namedtuple
import queue
from concurrent.futures import ThreadPoolExecutor
import concurrent.futures

CableInfo = namedtuple('CableInfo', [
    'playback_name', 'capture_name', 'num_channels',
    'playback_idx', 'capture_idx'
])


def discover_multichannel_cables():
    """Discover WASAPI cables with >2 channels for parallel benchmarking.

    Returns list of CableInfo sorted by name (Cable A first, then B, etc.).
    """
    import sounddevice as sd
    devices = sd.query_devices()
    hostapis = sd.query_hostapis()

    # Find WASAPI hostapi index
    wasapi_idx = None
    for i, h in enumerate(hostapis):
        if 'WASAPI' in h['name']:
            wasapi_idx = i
            break
    if wasapi_idx is None:
        return []

    # Find multi-channel CABLE playback/capture pairs
    playbacks = {}  # key: cable letter (A, B, ...) -> device info
    captures = {}

    for i, d in enumerate(devices):
        if d['hostapi'] != wasapi_idx:
            continue
        name = d['name']
        # Playback: "CABLE-A In 16ch (VB-Audio Virtual Cable A)"
        if 'CABLE-' in name and 'In' in name and d['max_output_channels'] > 2:
            letter = name.split('CABLE-')[1][0]  # 'A', 'B', etc.
            playbacks[letter] = (i, d)
        # Capture: "CABLE-A Output (VB-Audio Virtual Cable A)"
        if 'CABLE-' in name and 'Output' in name and d['max_input_channels'] > 2:
            letter = name.split('CABLE-')[1][0]
            captures[letter] = (i, d)

    cables = []
    for letter in sorted(playbacks.keys()):
        if letter not in captures:
            continue
        pb_idx, pb_dev = playbacks[letter]
        cap_idx, cap_dev = captures[letter]
        nch = min(pb_dev['max_output_channels'], cap_dev['max_input_channels'])
        cables.append(CableInfo(
            playback_name=pb_dev['name'],
            capture_name=cap_dev['name'],
            num_channels=nch,
            playback_idx=pb_idx,
            capture_idx=cap_idx,
        ))
    return cables


class BenchmarkWorker:
    """One worker = one audio channel. Pulls work items from a shared queue."""

    def __init__(self, worker_id, cable, channel,
                 results_queue, args, log_dir):
        self.worker_id = worker_id
        self.cable = cable
        self.channel = channel
        self.rsp_port = 7002 + worker_id * 10
        self.cmd_port = 7006 + worker_id * 10
        self.vb_in = cable.capture_name
        self.vb_out = cable.playback_name
        self.results = results_queue
        self.args = args
        self.log_dir = log_dir
        self.items_done = 0

    def _kill_own_ports(self):
        """Kill any Mercury processes listening on this worker's ports."""
        for port in (self.rsp_port, self.cmd_port):
            try:
                result = subprocess.run(
                    ['netstat', '-ano'], capture_output=True, text=True, timeout=5)
                for line in result.stdout.splitlines():
                    if f':{port} ' in line and 'LISTENING' in line:
                        parts = line.split()
                        pid = int(parts[-1])
                        if pid > 0:
                            subprocess.run(['taskkill', '/F', '/PID', str(pid)],
                                           capture_output=True, timeout=5)
            except Exception:
                pass

    def _start_session(self, cfg_id, is_nb, max_retries=2):
        """Start a Mercury session with retries. Returns session or None."""
        name = config_name(cfg_id, is_nb)
        extra_args = build_config_extra_args(is_nb, self.args.signal_dbfs)

        for attempt in range(max_retries):
            session = MercurySession(
                cfg_id, gearshift=False,
                mercury_path=self.args.mercury,
                rsp_port=self.rsp_port, cmd_port=self.cmd_port,
                vb_in=self.vb_in, vb_out=self.vb_out,
                audio_channel=self.channel, extra_args=extra_args)
            try:
                session.start(kill_all=False)

                if session.wait_connected(timeout=self.args.timeout):
                    if session.wait_data_batches(2, timeout=self.args.timeout):
                        return session
                    else:
                        print(f"  [W{self.worker_id}] NO_DATA for {name} (attempt {attempt+1})", flush=True)
                else:
                    print(f"  [W{self.worker_id}] CONN_FAIL for {name} (attempt {attempt+1})", flush=True)
            except Exception as e:
                print(f"  [W{self.worker_id}] START_ERROR for {name} (attempt {attempt+1}): {e}", flush=True)

            # Always clean up — kill session processes AND any zombies on our ports
            try:
                session.stop()
            except Exception:
                pass
            self._kill_own_ports()

            if attempt < max_retries - 1:
                time.sleep(5)

        return None

    def run(self, work_queue):
        """Main loop — pull (config, is_nb, snr_list) work items until queue empty.

        Each work item is a full config with all SNR points. The worker connects once
        and sweeps all SNR levels, getting natural warmup on the first point.
        """
        # Stagger startup to avoid simultaneous Mercury launches
        time.sleep(self.worker_id * 2)

        consecutive_fails = 0
        MAX_CONSECUTIVE_FAILS = 3

        while True:
            try:
                cfg_id, is_nb, snr_list = work_queue.get_nowait()
            except queue.Empty:
                break

            name = config_name(cfg_id, is_nb)

            # Circuit breaker: stop if this worker keeps failing
            if consecutive_fails >= MAX_CONSECUTIVE_FAILS:
                print(f"  [W{self.worker_id}] CIRCUIT BREAKER: {consecutive_fails} consecutive "
                      f"failures, returning {name} to queue", flush=True)
                # Put item back for a healthy worker
                work_queue.put((cfg_id, is_nb, snr_list))
                work_queue.task_done()
                self._kill_own_ports()
                break
            use_auto_dur = getattr(self.args, 'auto_duration', False)
            measure_dur = (auto_measure_duration(cfg_id, is_nb, self.args.measure_duration)
                           if use_auto_dur else self.args.measure_duration)

            try:
                session = self._start_session(cfg_id, is_nb)
                if not session:
                    consecutive_fails += 1
                    # Connection failed — record zeros for all SNR points
                    for snr_db in snr_list:
                        self.results.put({
                            'config': cfg_id, 'config_name': name,
                            'snr_db': snr_db, 'rx_bytes': 0, 'duration_s': 0,
                            'throughput_bps': 0, 'bytes_per_min': 0,
                            'nacks': 0, 'breaks': 0, 'process_alive': False,
                            'worker': self.worker_id, 'status': 'CONN_FAIL'
                        })
                    work_queue.task_done()
                    continue

                consecutive_fails = 0  # Connected successfully
                print(f"  [W{self.worker_id}] Connected {name}, ch={self.channel}, "
                      f"{len(snr_list)} SNR points, measure={measure_dur:.0f}s", flush=True)

                # Warmup: run one high-SNR measurement to fill the ARQ pipeline.
                # NB OFDM frames take 10-20s to transit; without warmup the first
                # 1-2 measurement windows are empty and trigger false waterfall.
                session.silence_noise()
                warmup_dur = min(measure_dur, 45)
                warmup_result = session.measure_throughput(warmup_dur)
                print(f"  [W{self.worker_id}] {name} warmup: {warmup_result['rx_bytes']}B "
                      f"in {warmup_dur:.0f}s", flush=True)

                # Waterfall detection — only after first successful measurement
                wf_threshold = 4 if cfg_id >= 100 else 2
                zero_count = 0
                seen_data = False

                for snr_idx, snr_db in enumerate(snr_list):
                    if not session.is_alive():
                        print(f"  [W{self.worker_id}] CRASH {name} @ SNR={snr_db}", flush=True)
                        for remaining_snr in snr_list[snr_idx:]:
                            self.results.put({
                                'config': cfg_id, 'config_name': name,
                                'snr_db': remaining_snr, 'rx_bytes': 0, 'duration_s': 0,
                                'throughput_bps': 0, 'bytes_per_min': 0,
                                'nacks': 0, 'breaks': 0, 'process_alive': False,
                                'worker': self.worker_id, 'status': 'CRASH'
                            })
                        break

                    session.set_noise_snr(snr_db)
                    time.sleep(self.args.settle_time)

                    pre_stat = len(session.rsp_lines)
                    result = session.measure_throughput(measure_dur)
                    stats = session.get_stats(pre_stat)

                    session.silence_noise()

                    nacks = stats['nacks_rsp'] + stats['nacks_cmd']
                    breaks = stats['breaks_rsp'] + stats['breaks_cmd']
                    status = 'OK' if result['rx_bytes'] > 0 else 'ZERO'

                    self.results.put({
                        'config': cfg_id, 'config_name': name,
                        'snr_db': snr_db,
                        'rx_bytes': result['rx_bytes'],
                        'duration_s': round(result['duration_s'], 1),
                        'throughput_bps': round(result['throughput_bps'], 1),
                        'bytes_per_min': round(result['bytes_per_min'], 1),
                        'nacks': nacks, 'breaks': breaks,
                        'process_alive': session.is_alive(),
                        'worker': self.worker_id, 'status': status,
                    })

                    print(f"  [W{self.worker_id}] {name} SNR={snr_db:+.0f}dB: "
                          f"{result['bytes_per_min']:.0f} B/min "
                          f"({result['throughput_bps']:.0f} bps) [{status}]", flush=True)

                    self.items_done += 1

                    # Waterfall detection — only after first non-zero measurement
                    if result['rx_bytes'] > 0:
                        seen_data = True
                        zero_count = 0
                    elif seen_data:
                        zero_count += 1
                        if zero_count >= wf_threshold:
                            print(f"  [W{self.worker_id}] {name} WATERFALL at SNR={snr_db}", flush=True)
                            for remaining_snr in snr_list[snr_idx+1:]:
                                self.results.put({
                                    'config': cfg_id, 'config_name': name,
                                    'snr_db': remaining_snr, 'rx_bytes': 0, 'duration_s': 0,
                                    'throughput_bps': 0, 'bytes_per_min': 0,
                                    'nacks': 0, 'breaks': 0,
                                    'process_alive': session.is_alive(),
                                    'worker': self.worker_id, 'status': 'WATERFALL'
                                })
                            break

                    if snr_idx < len(snr_list) - 1:
                        time.sleep(self.args.settle_time)

                # Save log and stop session
                session.silence_noise()
                session.save_log(os.path.join(self.log_dir, f'w{self.worker_id}_{name}.log'))
                session.stop()

            except Exception as e:
                consecutive_fails += 1
                print(f"  [W{self.worker_id}] ERROR on {name}: {e}", flush=True)
                # Kill any zombie processes on our ports
                self._kill_own_ports()
                for snr_db in snr_list:
                    self.results.put({
                        'config': cfg_id, 'config_name': name,
                        'snr_db': snr_db, 'rx_bytes': 0, 'duration_s': 0,
                        'throughput_bps': 0, 'bytes_per_min': 0,
                        'nacks': 0, 'breaks': 0, 'process_alive': False,
                        'worker': self.worker_id, 'status': f'ERROR:{e}'
                    })

            work_queue.task_done()

        print(f"  [W{self.worker_id}] Done ({self.items_done} items)", flush=True)


def run_parallel_sweep(args):
    """Parallel multi-channel SNR sweep using work queue architecture."""
    default_nb = getattr(args, 'narrowband', False)
    config_specs = parse_config_spec(args.configs, default_nb=default_nb)
    snr_levels = []
    snr = args.snr_start
    while snr >= args.snr_stop:
        snr_levels.append(snr)
        snr += args.snr_step

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(args.output_dir, exist_ok=True)
    csv_path = os.path.join(args.output_dir, f'benchmark_parallel_{ts}.csv')
    chart_path = os.path.join(args.output_dir, f'benchmark_parallel_{ts}.png')
    log_dir = os.path.join(args.output_dir, f'logs_parallel_{ts}')
    os.makedirs(log_dir, exist_ok=True)

    # Discover multi-channel cables
    cables = discover_multichannel_cables()
    if not cables:
        print("[ERROR] No multi-channel WASAPI cables found.")
        print("        Configure VB-Cable A/B to 16 channels in Windows Sound Settings.")
        sys.exit(1)

    total_channels = sum(c.num_channels for c in cables)
    max_workers = getattr(args, 'max_workers', total_channels) or total_channels
    max_workers = min(max_workers, total_channels)

    has_nb = any(nb for _, nb in config_specs)
    has_wb = any(not nb for _, nb in config_specs)
    mode_str = "NB+WB" if (has_nb and has_wb) else ("NB" if has_nb else "WB")
    use_auto_dur = getattr(args, 'auto_duration', False)

    # Generate work items: (config_id, is_nb, snr_list)
    # Each work item is one config with ALL its SNR points.
    # Worker connects once, sweeps all points — natural warmup on first point.
    work_items = [(cfg_id, is_nb, list(snr_levels)) for cfg_id, is_nb in config_specs]
    total_points = len(config_specs) * len(snr_levels)

    # Limit workers to number of configs (no benefit from more)
    max_workers = min(max_workers, len(work_items))

    print(f"{'='*70}")
    print(f"Mercury Benchmark -- PARALLEL SNR Sweep ({mode_str})")
    print(f"{'='*70}")
    print(f"Cables discovered: {len(cables)}")
    for c in cables:
        print(f"  {c.playback_name} <-> {c.capture_name} ({c.num_channels}ch)")
    print(f"Workers: {max_workers} (across {total_channels} total channels)")
    print(f"Cable signal level: {args.signal_dbfs:.1f} dBFS")
    print(f"Configs ({len(config_specs)}):")
    for cfg_id, is_nb in config_specs:
        name = config_name(cfg_id, is_nb)
        bps = config_max_bps(cfg_id, is_nb)
        dur = auto_measure_duration(cfg_id, is_nb, args.measure_duration) if use_auto_dur else args.measure_duration
        print(f"  {name:16s} max={bps:5d} bps  measure={dur:.0f}s")
    print(f"SNR: {snr_levels[0]}dB -> {snr_levels[-1]}dB (step {args.snr_step}dB, {len(snr_levels)} points)")
    print(f"Work items: {len(work_items)} configs ({total_points} total SNR points)")
    avg_dur = sum(auto_measure_duration(c, n, args.measure_duration) if use_auto_dur else args.measure_duration
                  for c, n in config_specs) / max(len(config_specs), 1)
    # Estimate: slowest config determines wall time (all run in parallel)
    max_config_dur = max(
        len(snr_levels) * (auto_measure_duration(c, n, args.measure_duration)
                           if use_auto_dur else args.measure_duration + args.settle_time)
        for c, n in config_specs)
    est_time = max_config_dur + 30  # + connect overhead
    print(f"Estimated time: {est_time/60:.0f} min (limited by slowest config)")
    print(f"Output: {csv_path}")
    print()

    # Kill any existing mercury processes first
    os.system("taskkill /F /IM mercury.exe 2>nul >nul")
    time.sleep(2)

    # Create work queue — one item per config
    work_queue = queue.Queue()
    for item in work_items:
        work_queue.put(item)

    results_queue = queue.Queue()

    # Create workers — noise injection is now internal to Mercury (-Z flag)
    # via NOISESNR TCP command, no external noise injector needed.
    workers = []

    for cable_idx, cable in enumerate(cables):
        if len(workers) >= max_workers:
            break
        for ch in range(cable.num_channels):
            if len(workers) >= max_workers:
                break
            w = BenchmarkWorker(len(workers), cable, ch,
                                results_queue, args, log_dir)
            workers.append(w)

    print(f"[PARALLEL] {len(work_items)} work items, {len(workers)} workers")
    print(f"[PARALLEL] Starting...\n")

    try:
        # Launch all workers
        with ThreadPoolExecutor(max_workers=len(workers)) as pool:
            futures = {pool.submit(w.run, work_queue): w for w in workers}

            # Wait with progress reporting
            done_count = 0
            total = len(work_items)
            last_report = time.time()
            for f in concurrent.futures.as_completed(futures):
                w = futures[f]
                try:
                    f.result()
                except Exception as e:
                    print(f"  [W{w.worker_id}] Worker crashed: {e}", flush=True)
                done_count += w.items_done
                now = time.time()
                if now - last_report > 10:
                    pending = results_queue.qsize()
                    print(f"  [PROGRESS] ~{pending}/{total} results collected", flush=True)
                    last_report = now

        # Collect results
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())

        # Sort by (config, snr_db) for consistent CSV output
        results.sort(key=lambda r: (r['config'], r['config_name'], r['snr_db']))

        # Write CSV — use same format as sequential sweep for compatibility
        fieldnames = ['config', 'config_name', 'snr_db', 'rx_bytes', 'duration_s',
                      'throughput_bps', 'bytes_per_min', 'nacks', 'breaks',
                      'process_alive', 'worker', 'status']
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"\n[CSV] Saved to {csv_path}")

        # Generate chart — reuse existing sweep chart generator
        generate_sweep_chart(csv_path, chart_path)

        # Summary
        print(f"\n{'='*70}")
        print(f"PARALLEL SWEEP COMPLETE ({mode_str})")
        print(f"{'='*70}")
        for cfg_id, is_nb in config_specs:
            name = config_name(cfg_id, is_nb)
            cfg_rows = [r for r in results if r['config_name'] == name]
            max_bpm = max((r['bytes_per_min'] for r in cfg_rows), default=0)
            waterfall_snr = None
            for r in sorted(cfg_rows, key=lambda x: x['snr_db'], reverse=True):
                if r['bytes_per_min'] > 0:
                    waterfall_snr = r['snr_db']
            fails = sum(1 for r in cfg_rows if r.get('status') not in ('OK', 'ZERO'))
            if waterfall_snr is not None:
                extra = f" ({fails} fails)" if fails else ""
                print(f"  {name:16s}: peak {max_bpm:.0f} B/min, waterfall ~{waterfall_snr:.0f} dB{extra}")
            else:
                print(f"  {name:16s}: no throughput measured")

        total_ok = sum(1 for r in results if r.get('status') == 'OK')
        total_zero = sum(1 for r in results if r.get('status') == 'ZERO')
        total_fail = sum(1 for r in results if r.get('status') not in ('OK', 'ZERO'))
        print(f"\n  Totals: {total_ok} OK, {total_zero} ZERO, {total_fail} failures")
        print(f"{'='*70}")

    finally:
        # Safety cleanup
        os.system("taskkill /F /IM mercury.exe 2>nul >nul")


# ============================================================
# Sub-command: sweep
# ============================================================

def run_sweep(args):
    """Fixed-config SNR sweep — measures throughput at each SNR level per config."""
    default_nb = getattr(args, 'narrowband', False)
    config_specs = parse_config_spec(args.configs, default_nb=default_nb)
    snr_levels = []
    snr = args.snr_start
    while snr >= args.snr_stop:
        snr_levels.append(snr)
        snr += args.snr_step  # step is negative

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(args.output_dir, exist_ok=True)
    csv_path = os.path.join(args.output_dir, f'benchmark_sweep_{ts}.csv')
    chart_path = os.path.join(args.output_dir, f'benchmark_sweep_{ts}.png')
    log_dir = os.path.join(args.output_dir, f'logs_{ts}')
    os.makedirs(log_dir, exist_ok=True)

    has_nb = any(nb for _, nb in config_specs)
    has_wb = any(not nb for _, nb in config_specs)
    mode_str = "NB+WB" if (has_nb and has_wb) else ("NB" if has_nb else "WB")
    use_auto_dur = getattr(args, 'auto_duration', False)

    print(f"{'='*70}")
    print(f"Mercury Benchmark -- SNR Sweep ({mode_str})")
    print(f"{'='*70}")
    print(f"Cable signal level: {args.signal_dbfs:.1f} dBFS")
    print(f"Configs ({len(config_specs)}):")
    for cfg_id, is_nb in config_specs:
        name = config_name(cfg_id, is_nb)
        bps = config_max_bps(cfg_id, is_nb)
        dur = auto_measure_duration(cfg_id, is_nb, args.measure_duration) if use_auto_dur else args.measure_duration
        print(f"  {name:16s} max={bps:5d} bps  measure={dur:.0f}s")
    print(f"SNR levels: {snr_levels[0]}dB -> {snr_levels[-1]}dB (step {args.snr_step}dB, {len(snr_levels)} points)")
    print(f"Auto-duration: {'ON' if use_auto_dur else 'OFF'}")
    print(f"Settle time: {args.settle_time}s between points")
    total_points = len(config_specs) * len(snr_levels)
    avg_dur = sum(auto_measure_duration(c, n, args.measure_duration) if use_auto_dur else args.measure_duration
                  for c, n in config_specs) / max(len(config_specs), 1)
    est_time = len(config_specs) * (len(snr_levels) * (avg_dur + args.settle_time) + 30)
    print(f"Total points: {total_points} (est {est_time/3600:.1f} hours)")
    print(f"Output: {csv_path}")
    print()

    # Noise injection is now internal to Mercury via NOISESNR TCP command.

    # CSV header
    results = []
    fieldnames = ['config', 'config_name', 'snr_db', 'rx_bytes', 'duration_s',
                  'throughput_bps', 'bytes_per_min', 'nacks', 'breaks', 'process_alive']

    try:
        for cfg_idx, (cfg, is_nb) in enumerate(config_specs):
            cfg_name = config_name(cfg, is_nb)
            extra_args = build_config_extra_args(is_nb, args.signal_dbfs)
            measure_dur = (auto_measure_duration(cfg, is_nb, args.measure_duration)
                           if use_auto_dur else args.measure_duration)
            native_dbfs = NoiseInjector.DEFAULT_SIGNAL_DBFS_NB if is_nb else NoiseInjector.DEFAULT_SIGNAL_DBFS
            atten_db = args.signal_dbfs - native_dbfs

            print(f"\n{'='*70}")
            print(f"[{cfg_idx+1}/{len(config_specs)}] Starting {cfg_name} (config={cfg})")
            print(f"  TX {atten_db:.1f}dB -> cable {args.signal_dbfs:.1f}dBFS, "
                  f"RX +{-atten_db:.1f}dB, measure={measure_dur:.0f}s")
            print(f"{'='*70}")

            session = MercurySession(cfg, gearshift=False, mercury_path=args.mercury,
                                     extra_args=extra_args)
            try:
                session.start()

                if not session.wait_connected(timeout=args.timeout):
                    print(f"  [ERROR] Connection failed for {cfg_name}")
                    # Dump key diagnostic lines from modem output
                    for role, lines in [('RSP', session.rsp_lines), ('CMD', session.cmd_lines)]:
                        decode_lines = [l for l in lines if 'RX-DECODE' in l or 'NO-PREAMBLE' in l
                                        or 'CMD-ACK-PAT' in l or 'ACK-CTRL' in l or 'CONNECTED' in l
                                        or 'link_status:Conn' in l or 'START_CONN' in l
                                        or 'TEST_CONN' in l or 'NAck' in l
                                        or 'ACK-RX' in l or 'CMD-RX' in l or 'CFG]' in l
                                        or 'PHY]' in l or 'recv_timeout' in l
                                        or 'ACK-DET' in l or 'ACK-SEND' in l
                                        or 'ACK-DIAG' in l or 'ACK-WINDOW' in l]
                        # Also find ACK-DIAG lines separately (they appear early)
                        diag_lines = [l for l in lines if 'ACK-DIAG' in l]
                        if diag_lines:
                            print(f"  [ACK-DIAG-{role}] ({len(diag_lines)} lines):")
                            for dl in diag_lines[:30]:
                                print(f"    {dl.rstrip()}")
                        if decode_lines:
                            print(f"  [DIAG-{role}] ({len(decode_lines)} key lines, showing last 60):")
                            for dl in decode_lines[-60:]:
                                print(f"    {dl.rstrip()}")
                        else:
                            print(f"  [DIAG-{role}] No key diagnostic lines in {len(lines)} total lines")
                    sys.stdout.flush()
                    for snr_db in snr_levels:
                        results.append({
                            'config': cfg, 'config_name': cfg_name, 'snr_db': snr_db,
                            'rx_bytes': 0, 'duration_s': 0, 'throughput_bps': 0,
                            'bytes_per_min': 0, 'nacks': 0, 'breaks': 0, 'process_alive': False,
                        })
                    continue

                print(f"  [OK] Connected. Waiting for 2 data batches...")
                if not session.wait_data_batches(2, timeout=args.timeout):
                    print(f"  [ERROR] Data exchange not started for {cfg_name}")
                    continue

                pre_rx = session.get_rx_bytes()
                rx_alive = session._rx_thread.is_alive() if session._rx_thread else False
                print(f"  [OK] Data flowing. RX thread alive={rx_alive}, pre-measurement bytes={pre_rx}")
                print(f"  Starting SNR sweep...\n")

                zero_count = 0
                stat_baseline = len(session.rsp_lines)

                # Waterfall threshold: ROBUST modes need more patience (slow ARQ)
                if args.no_waterfall:
                    wf_threshold = 0  # disabled
                elif args.waterfall_threshold is not None:
                    wf_threshold = args.waterfall_threshold
                elif cfg >= 100:
                    wf_threshold = 4  # ROBUST modes: 4 consecutive zeros
                else:
                    wf_threshold = 2  # OFDM modes: 2 consecutive zeros

                for snr_idx, snr_db in enumerate(snr_levels):
                    if not session.is_alive():
                        print(f"  [CRASH] Process died at SNR={snr_db}dB", flush=True)
                        # Record remaining as dead
                        for remaining_snr in snr_levels[snr_idx:]:
                            results.append({
                                'config': cfg, 'config_name': cfg_name, 'snr_db': remaining_snr,
                                'rx_bytes': 0, 'duration_s': 0, 'throughput_bps': 0,
                                'bytes_per_min': 0, 'nacks': 0, 'breaks': 0, 'process_alive': False,
                            })
                        break

                    session.set_noise_snr(snr_db)

                    pre_stat = len(session.rsp_lines)
                    result = session.measure_throughput(measure_dur)
                    sys.stdout.flush()
                    stats = session.get_stats(pre_stat)

                    session.silence_noise()

                    nacks = stats['nacks_rsp'] + stats['nacks_cmd']
                    breaks = stats['breaks_rsp'] + stats['breaks_cmd']

                    row = {
                        'config': cfg, 'config_name': cfg_name, 'snr_db': snr_db,
                        'rx_bytes': result['rx_bytes'], 'duration_s': round(result['duration_s'], 1),
                        'throughput_bps': round(result['throughput_bps'], 1),
                        'bytes_per_min': round(result['bytes_per_min'], 1),
                        'nacks': nacks, 'breaks': breaks,
                        'process_alive': session.is_alive(),
                    }
                    results.append(row)

                    status = "OK" if result['rx_bytes'] > 0 else "ZERO"
                    print(f"  [{snr_idx+1}/{len(snr_levels)}] SNR={snr_db:+.0f}dB: "
                          f"{result['bytes_per_min']:.0f} B/min "
                          f"({result['throughput_bps']:.0f} bps), "
                          f"{result['rx_bytes']}B in {result['duration_s']:.0f}s, "
                          f"NAcks={nacks} [{status}]", flush=True)

                    # Debug: dump Mercury output on zero throughput
                    if result['rx_bytes'] == 0 and snr_idx == 0:
                        # Extract key stats
                        def _last_val(lines, key):
                            for l in reversed(lines):
                                if key in l:
                                    try: return l.split('=')[-1].strip()
                                    except: pass
                            return '?'
                        rsp, cmd = session.rsp_lines, session.cmd_lines
                        print(f"  [DEBUG] RSP: nAcked_data={_last_val(rsp,'nAcked_data')}, "
                              f"nNAcked_data={_last_val(rsp,'nNAcked_data')}, "
                              f"nAcks_sent_data={_last_val(rsp,'nAcks_sent_data')}")
                        print(f"  [DEBUG] CMD: nAcked_data={_last_val(cmd,'nAcked_data')}, "
                              f"nNAcked_data={_last_val(cmd,'nNAcked_data')}")
                        # Check for key events
                        for pat in ['DBG-COPY', 'BLOCK_END', 'end of block', 'end of file',
                                    'DBG-RSP-TX', 'Acknowledging data', 'Receiving data',
                                    'DBG-DATA', 'DBG-FILL', 'DBG-BLOCKEND', 'DBG-BLKWAIT']:
                            rc = sum(1 for l in rsp if pat in l)
                            cc = sum(1 for l in cmd if pat in l)
                            if rc or cc:
                                print(f"  [DEBUG] '{pat}': RSP={rc}, CMD={cc}")
                        print(f"  [DEBUG] Lines: RSP={len(rsp)}, CMD={len(cmd)}")
                        # Last nAcked_data values
                        acked = [l for l in cmd if 'nAcked_data' in l]
                        print(f"  [DEBUG] CMD nAcked_data lines: {len(acked)}")
                        for b in acked[-3:]:
                            print(f"    {b.rstrip()}")
                        # DBG-DATA/BLKWAIT state dumps
                        for tag in ['DBG-DATA', 'DBG-BLKWAIT', 'DBG-FILL', 'DBG-BLOCKEND']:
                            hits = [l for l in cmd if tag in l]
                            if hits:
                                print(f"  [DEBUG] CMD {tag} ({len(hits)} lines):")
                                for h in hits[:5]:
                                    print(f"    {h.rstrip()}")
                        sys.stdout.flush()

                    # Waterfall detection
                    if result['rx_bytes'] == 0:
                        zero_count += 1
                        if wf_threshold > 0 and zero_count >= wf_threshold:
                            print(f"  [WATERFALL] {wf_threshold} consecutive zero-throughput points -- skipping remaining SNRs")
                            for remaining_snr in snr_levels[snr_idx+1:]:
                                results.append({
                                    'config': cfg, 'config_name': cfg_name, 'snr_db': remaining_snr,
                                    'rx_bytes': 0, 'duration_s': 0, 'throughput_bps': 0,
                                    'bytes_per_min': 0, 'nacks': 0, 'breaks': 0,
                                    'process_alive': session.is_alive(),
                                })
                            break
                    else:
                        zero_count = 0

                    # Settle between points
                    if snr_idx < len(snr_levels) - 1:
                        time.sleep(args.settle_time)

            finally:
                session.save_log(os.path.join(log_dir, f'{cfg_name}.log'))
                session.stop()

        # Write CSV
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"\n[CSV] Saved to {csv_path}")

        # Generate chart
        generate_sweep_chart(csv_path, chart_path)

        # Summary
        print(f"\n{'='*70}")
        print(f"SWEEP COMPLETE ({mode_str})")
        print(f"{'='*70}")
        for cfg_id, is_nb in config_specs:
            name = config_name(cfg_id, is_nb)
            cfg_rows = [r for r in results if r['config_name'] == name]
            max_bpm = max((r['bytes_per_min'] for r in cfg_rows), default=0)
            waterfall_snr = None
            for r in sorted(cfg_rows, key=lambda x: x['snr_db'], reverse=True):
                if r['bytes_per_min'] > 0:
                    waterfall_snr = r['snr_db']
            if waterfall_snr is not None:
                print(f"  {name:16s}: peak {max_bpm:.0f} B/min, waterfall ~{waterfall_snr:.0f} dB")
            else:
                print(f"  {name:16s}: no throughput measured")

    finally:
        pass  # Noise cleanup handled by Mercury process termination


# ============================================================
# Sub-command: stress
# ============================================================

def run_stress(args):
    """Random noise bursts — tests gearshift, BREAK, and recovery."""
    rng = random.Random(args.seed if hasattr(args, 'seed') and args.seed else None)

    # Generate noise schedule
    schedule = []
    for i in range(args.num_bursts):
        dur = rng.uniform(args.min_dur, args.max_dur)
        snr = rng.uniform(args.snr_low, args.snr_high)
        silence = rng.uniform(30, 90) if i < args.num_bursts - 1 else 0
        schedule.append({'burst': i+1, 'duration': dur, 'snr_db': snr, 'silence_after': silence})

    total_noise = sum(s['duration'] for s in schedule)
    total_silence = sum(s['silence_after'] for s in schedule)
    total_est = total_noise + total_silence + 300  # +300 for turboshift + setup

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(args.output_dir, exist_ok=True)
    csv_path = os.path.join(args.output_dir, f'benchmark_stress_{ts}.csv')
    timeline_path = os.path.join(args.output_dir, f'benchmark_stress_timeline_{ts}.csv')
    chart_path = os.path.join(args.output_dir, f'benchmark_stress_{ts}.png')
    log_path = os.path.join(args.output_dir, f'benchmark_stress_{ts}.log')

    nb = getattr(args, 'narrowband', False)
    names = get_config_names(nb)

    print(f"{'='*70}")
    print(f"Mercury Benchmark -- Stress Test{' (NARROWBAND)' if nb else ''}")
    print(f"{'='*70}")
    print(f"Start config: {names.get(args.start_config, str(args.start_config))}")
    print(f"Bursts: {args.num_bursts}")
    print(f"Noise schedule:")
    for s in schedule:
        print(f"  Burst {s['burst']}: SNR={s['snr_db']:.1f}dB for {s['duration']:.0f}s, "
              f"then {s['silence_after']:.0f}s silence")
    print(f"Total noise time: {total_noise:.0f}s, silence: {total_silence:.0f}s")
    print(f"Estimated total: {total_est/60:.0f} min")
    print()

    extra_args = build_extra_args(args)

    session = MercurySession(args.start_config, gearshift=True, mercury_path=args.mercury,
                              extra_args=extra_args)
    burst_results = []
    timeline = []
    t_origin = None  # set after connection

    try:
        session.start()

        if not session.wait_connected(timeout=args.timeout):
            print("[ERROR] Connection failed")
            return

        t_origin = time.time()
        print(f"[OK] Connected at T+0s")

        # Wait for turboshift
        print("[TURBO] Waiting for turboshift to complete...")
        if not session.wait_turboshift(timeout=args.timeout):
            print("[WARN] Turboshift did not complete -- proceeding anyway")
        else:
            print(f"[TURBO] Complete at T+{time.time()-t_origin:.0f}s")

        # Wait for data
        if not session.wait_data_batches(3, timeout=120):
            print("[ERROR] Data exchange not started")
            return

        print(f"[OK] Data flowing at T+{time.time()-t_origin:.0f}s")
        print(f"[OK] Config: {session.get_current_config()}")
        print()

        # Timeline recording thread
        timeline_stop = threading.Event()

        def timeline_recorder():
            while not timeline_stop.is_set():
                elapsed = time.time() - t_origin
                rx = session.get_rx_bytes()
                timeline.append({
                    'elapsed_s': round(elapsed, 1),
                    'rx_bytes_cumulative': rx,
                    'noise_on': 1 if noise.playing else 0,
                    'noise_snr': noise.snr_db if noise.playing else '',
                })
                time.sleep(1)

        tl_thread = threading.Thread(target=timeline_recorder, daemon=True)
        tl_thread.start()

        # Execute noise schedule
        for s in schedule:
            burst_num = s['burst']
            burst_snr = s['snr_db']
            burst_dur = s['duration']
            silence = s['silence_after']

            print(f"\n--- Burst {burst_num}/{args.num_bursts}: SNR={burst_snr:.1f}dB for {burst_dur:.0f}s ---")

            if not session.is_alive():
                print(f"[CRASH] Process died before burst {burst_num}")
                burst_results.append({
                    'burst': burst_num, 'snr_db': round(burst_snr, 1),
                    'duration': round(burst_dur, 0),
                    'bytes_during_noise': 0, 'bytes_during_recovery': 0,
                    'nacks': 0, 'breaks': 0, 'geardown': 0,
                    'config_at_end': 'DEAD', 'process_alive': False,
                })
                continue

            pre_stat = len(session.rsp_lines)
            pre_bytes = session.get_rx_bytes()

            # Noise ON
            session.set_noise_snr(burst_snr)
            t_burst = time.time()

            # Monitor during noise
            last_report = t_burst
            while time.time() - t_burst < burst_dur:
                if not session.is_alive():
                    print(f"  [CRASH] Process died during burst!")
                    break
                now = time.time()
                if now - last_report >= 15:
                    stats = session.get_stats(pre_stat)
                    rx_during = session.get_rx_bytes() - pre_bytes
                    print(f"  T+{now-t_origin:.0f}s | "
                          f"RX={rx_during}B NAcks={stats['nacks_rsp']+stats['nacks_cmd']} "
                          f"BREAKs={stats['breaks_rsp']+stats['breaks_cmd']} "
                          f"Drops={stats['break_drops']} "
                          f"GearDown={stats['geardown']}")
                    last_report = now
                time.sleep(1)

            # Noise OFF
            session.silence_noise()
            bytes_during_noise = session.get_rx_bytes() - pre_bytes

            print(f"  Noise off. RX during noise: {bytes_during_noise}B. "
                  f"Recovery for {silence:.0f}s...")

            # Recovery period
            pre_recovery_bytes = session.get_rx_bytes()
            t_recovery = time.time()
            while time.time() - t_recovery < silence:
                if not session.is_alive():
                    break
                time.sleep(1)
            bytes_during_recovery = session.get_rx_bytes() - pre_recovery_bytes

            stats = session.get_stats(pre_stat)
            burst_results.append({
                'burst': burst_num, 'snr_db': round(burst_snr, 1),
                'duration': round(burst_dur, 0),
                'bytes_during_noise': bytes_during_noise,
                'bytes_during_recovery': bytes_during_recovery,
                'nacks': stats['nacks_rsp'] + stats['nacks_cmd'],
                'breaks': stats['breaks_rsp'] + stats['breaks_cmd'],
                'break_drops': stats['break_drops'],
                'geardown': stats['geardown'],
                'config_at_end': session.get_current_config(),
                'process_alive': session.is_alive(),
            })

            print(f"  Burst {burst_num} done: noise={bytes_during_noise}B, "
                  f"recovery={bytes_during_recovery}B, "
                  f"NAcks={stats['nacks_rsp']+stats['nacks_cmd']}, "
                  f"BREAKs={stats['breaks_rsp']+stats['breaks_cmd']}, "
                  f"Drops={stats['break_drops']}, "
                  f"GearDown={stats['geardown']}")

        # Final recovery
        print(f"\n[POST] Final 60s recovery period...")
        time.sleep(60)

        timeline_stop.set()

        # Write CSVs
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'burst', 'snr_db', 'duration', 'bytes_during_noise',
                'bytes_during_recovery', 'nacks', 'breaks', 'break_drops',
                'geardown', 'config_at_end', 'process_alive'])
            writer.writeheader()
            writer.writerows(burst_results)

        with open(timeline_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=[
                'elapsed_s', 'rx_bytes_cumulative', 'noise_on', 'noise_snr'])
            writer.writeheader()
            writer.writerows(timeline)

        print(f"\n[CSV] Burst data: {csv_path}")
        print(f"[CSV] Timeline:   {timeline_path}")

        generate_stress_chart(timeline_path, chart_path)

        # Summary
        total_rx = session.get_rx_bytes()
        total_stats = session.get_stats()
        print(f"\n{'='*70}")
        print(f"STRESS TEST COMPLETE")
        print(f"{'='*70}")
        print(f"  Total RX: {total_rx} bytes")
        print(f"  Bursts: {args.num_bursts}")
        print(f"  NAcks total: {total_stats['nacks_rsp']+total_stats['nacks_cmd']}")
        print(f"  BREAKs total: {total_stats['breaks_rsp']+total_stats['breaks_cmd']}")
        print(f"  BREAK drops: {total_stats['break_drops']}")
        print(f"  BREAK recovery: {total_stats['break_recovery']}")
        print(f"  Gear down (ladder): {total_stats['geardown']}")
        print(f"  Gear up (ladder): {total_stats['gearup']}")
        print(f"  Config changes: {total_stats['config_changes']}")
        print(f"  Process alive: {session.is_alive()}")

        if session.is_alive():
            print(f"\n  VERDICT: PASS")
        else:
            for p in session.procs:
                if p.returncode is not None and p.returncode < 0:
                    rc = p.returncode & 0xFFFFFFFF
                    print(f"  Exit code: 0x{rc:08X}")
            print(f"\n  VERDICT: FAIL (process crashed)")

        print(f"{'='*70}")

    finally:
        session.save_log(log_path)
        session.stop()


# ============================================================
# Sub-command: adaptive
# ============================================================

def run_adaptive(args):
    """Gearshift SNR sweep — single session, sweeps SNR down and optionally back up."""
    snr_levels_down = []
    snr = args.snr_start
    while snr >= args.snr_stop:
        snr_levels_down.append(snr)
        snr += args.snr_step

    snr_levels = list(snr_levels_down)
    if args.round_trip:
        snr_levels += list(reversed(snr_levels_down[:-1]))  # sweep back up

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(args.output_dir, exist_ok=True)
    csv_path = os.path.join(args.output_dir, f'benchmark_adaptive_{ts}.csv')
    chart_path = os.path.join(args.output_dir, f'benchmark_adaptive_{ts}.png')
    log_path = os.path.join(args.output_dir, f'benchmark_adaptive_{ts}.log')

    nb = getattr(args, 'narrowband', False)

    print(f"{'='*70}")
    print(f"Mercury Benchmark -- Adaptive (Gearshift) SNR Sweep{' (NARROWBAND)' if nb else ''}")
    print(f"{'='*70}")
    print(f"SNR levels: {snr_levels} dB")
    print(f"Measurement duration: {args.measure_duration}s per point")
    print(f"Round trip: {'yes' if args.round_trip else 'no'}")
    est_time = len(snr_levels) * (args.measure_duration + args.settle_time) + 300
    print(f"Estimated time: {est_time/60:.0f} min")
    print()

    extra_args = build_extra_args(args)

    session = MercurySession(100, gearshift=True, mercury_path=args.mercury,
                              extra_args=extra_args)
    results = []
    fieldnames = ['snr_db', 'direction', 'rx_bytes', 'duration_s', 'throughput_bps',
                  'bytes_per_min', 'nacks', 'breaks', 'geardown', 'gearup',
                  'config_at_end', 'process_alive']

    try:
        session.start()

        if not session.wait_connected(timeout=args.timeout):
            print("[ERROR] Connection failed")
            return

        t_start = time.time()
        print(f"[OK] Connected")

        print("[TURBO] Waiting for turboshift...")
        if not session.wait_turboshift(timeout=args.timeout):
            print("[WARN] Turboshift did not complete")
        else:
            print(f"[TURBO] Complete at T+{time.time()-t_start:.0f}s")

        if not session.wait_data_batches(3, timeout=120):
            print("[ERROR] Data exchange not started")
            return

        print(f"[OK] Data flowing. Config: {session.get_current_config()}")
        print()

        half = len(snr_levels_down)

        for snr_idx, snr_db in enumerate(snr_levels):
            direction = "DOWN" if snr_idx < half else "UP"

            if not session.is_alive():
                print(f"[CRASH] Process died at SNR={snr_db}dB")
                for remaining_snr in snr_levels[snr_idx:]:
                    results.append({
                        'snr_db': remaining_snr, 'direction': direction,
                        'rx_bytes': 0, 'duration_s': 0, 'throughput_bps': 0,
                        'bytes_per_min': 0, 'nacks': 0, 'breaks': 0,
                        'geardown': 0, 'gearup': 0,
                        'config_at_end': 'DEAD', 'process_alive': False,
                    })
                break

            pre_stat = len(session.rsp_lines)
            session.set_noise_snr(snr_db)

            result = session.measure_throughput(args.measure_duration)
            stats = session.get_stats(pre_stat)

            session.silence_noise()

            row = {
                'snr_db': snr_db, 'direction': direction,
                'rx_bytes': result['rx_bytes'],
                'duration_s': round(result['duration_s'], 1),
                'throughput_bps': round(result['throughput_bps'], 1),
                'bytes_per_min': round(result['bytes_per_min'], 1),
                'nacks': stats['nacks_rsp'] + stats['nacks_cmd'],
                'breaks': stats['breaks_rsp'] + stats['breaks_cmd'],
                'geardown': stats['geardown'], 'gearup': stats['gearup'],
                'config_at_end': session.get_current_config(),
                'process_alive': session.is_alive(),
            }
            results.append(row)

            print(f"  [{snr_idx+1}/{len(snr_levels)}] {direction} SNR={snr_db:+.0f}dB: "
                  f"{result['bytes_per_min']:.0f} B/min, "
                  f"GearDown={stats['geardown']} GearUp={stats['gearup']} "
                  f"NAcks={row['nacks']}")

            if snr_idx < len(snr_levels) - 1:
                time.sleep(args.settle_time)

        # Write CSV
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"\n[CSV] Saved to {csv_path}")

        # Generate chart
        try:
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt

            down_rows = [r for r in results if r['direction'] == 'DOWN' and r['process_alive']]
            up_rows = [r for r in results if r['direction'] == 'UP' and r['process_alive']]

            fig, ax = plt.subplots(figsize=(14, 8))
            if down_rows:
                ax.plot([r['snr_db'] for r in down_rows],
                        [r['bytes_per_min'] for r in down_rows],
                        'bo-', linewidth=2, markersize=5, label='SNR decreasing')
            if up_rows:
                ax.plot([r['snr_db'] for r in up_rows],
                        [r['bytes_per_min'] for r in up_rows],
                        'rs-', linewidth=2, markersize=5, label='SNR increasing')

            ax.set_xlabel('SNR (dB in 4 kHz)', fontsize=12)
            ax.set_ylabel('Throughput (bytes/min)', fontsize=12)
            ax.set_title('Mercury Modem — Adaptive Throughput vs SNR', fontsize=14)
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            if any(r['bytes_per_min'] > 0 for r in results):
                ax.set_yscale('log')
            plt.tight_layout()
            plt.savefig(chart_path, dpi=150)
            print(f"[CHART] Saved to {chart_path}")
            plt.close()
        except ImportError:
            print("[CHART] matplotlib not available")

        # Summary
        print(f"\n{'='*70}")
        print(f"ADAPTIVE SWEEP COMPLETE")
        print(f"{'='*70}")
        total_stats = session.get_stats()
        print(f"  Process alive: {session.is_alive()}")
        print(f"  NAcks: {total_stats['nacks_rsp']+total_stats['nacks_cmd']}")
        print(f"  BREAKs: {total_stats['breaks_rsp']+total_stats['breaks_cmd']}")
        print(f"  Gear down: {total_stats['geardown']}")
        print(f"  Gear up: {total_stats['gearup']}")
        if session.is_alive():
            print(f"  VERDICT: PASS")
        else:
            print(f"  VERDICT: FAIL (process crashed)")
        print(f"{'='*70}")

    finally:
        session.save_log(log_path)
        session.stop()


# ============================================================
# BER — PLOT_PASSBAND waterfall curves
# ============================================================

def run_single_ber(mercury_path, config_id, is_nb, robust):
    """Run a single BER test and return list of (EsN0, BER) tuples."""
    cmd = [mercury_path, "-m", "PLOT_PASSBAND", "-s", str(config_id)]
    if robust:
        cmd.append("-R")
    if is_nb:
        cmd.append("-N")
    # Run headless (-n not needed for PLOT_PASSBAND, it exits after test)
    try:
        # NB configs have 5x longer symbols; high-rate WB configs need
        # more processing time.  900s is generous but prevents false timeouts.
        ber_timeout = 900 if is_nb else 600
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=ber_timeout,
                                errors='replace')
        points = []
        for line in result.stdout.splitlines():
            if ';' in line:
                parts = line.strip().split(';')
                if len(parts) == 2:
                    try:
                        esn0 = float(parts[0])
                        ber = float(parts[1])
                        points.append((esn0, ber))
                    except ValueError:
                        continue
        return points
    except subprocess.TimeoutExpired:
        print(f"  [BER] Timeout for config {config_id} ({'NB' if is_nb else 'WB'})")
        return []
    except Exception as e:
        print(f"  [BER] Error for config {config_id}: {e}")
        return []


def run_ber(args):
    """Run BER waterfall tests for all specified configs."""
    from concurrent.futures import ThreadPoolExecutor, as_completed

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'ber_{ts}')
    os.makedirs(output_dir, exist_ok=True)

    configs = parse_config_spec(getattr(args, 'configs', 'all-wb,all-nb'),
                                default_nb=getattr(args, 'narrowband', False))
    max_workers = getattr(args, 'max_workers', 8) or 8

    print(f"\n{'='*70}")
    print(f"BER WATERFALL TEST — {len(configs)} configs, {max_workers} parallel workers")
    print(f"Output: {output_dir}")
    print(f"{'='*70}\n")

    # Run all BER tests in parallel
    all_results = {}  # (config_id, is_nb) -> [(esn0, ber), ...]
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        for config_id, is_nb in configs:
            robust = config_id >= 100
            name = config_name(config_id, is_nb)
            f = executor.submit(run_single_ber, args.mercury, config_id, is_nb, robust)
            futures[f] = (config_id, is_nb, name)

        for f in as_completed(futures):
            config_id, is_nb, name = futures[f]
            points = f.result()
            all_results[(config_id, is_nb)] = points
            status = f"{len(points)} points" if points else "FAILED"
            print(f"  {name}: {status}")

    # Write CSV
    csv_path = os.path.join(output_dir, 'ber_results.csv')
    with open(csv_path, 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['config', 'config_name', 'bandwidth', 'esn0_db', 'ber'])
        for (config_id, is_nb), points in sorted(all_results.items()):
            name = config_name(config_id, is_nb)
            bw = 'NB' if is_nb else 'WB'
            for esn0, ber in points:
                w.writerow([config_id, name, bw, f'{esn0:.1f}', f'{ber:.6e}'])
    print(f"\nCSV: {csv_path}")

    # Generate charts
    for bw_label, bw_filter in [('WB', False), ('NB', True)]:
        chart_path = os.path.join(output_dir, f'ber_waterfall_{bw_label.lower()}.png')
        filtered = {k: v for k, v in all_results.items() if k[1] == bw_filter and v}
        if filtered:
            generate_ber_chart(filtered, chart_path, f'BER Waterfall — {bw_label}')
            print(f"Chart: {chart_path}")

    print(f"\nBER test complete. {len(all_results)} configs tested.")
    return output_dir


def generate_ber_chart(results, output_path, title):
    """Generate BER vs Es/N0 waterfall chart."""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
        from matplotlib.cm import get_cmap

        fig, ax = plt.subplots(figsize=(12, 8))
        cmap = get_cmap('viridis', len(results))

        for i, ((config_id, is_nb), points) in enumerate(sorted(results.items())):
            if not points:
                continue
            esn0s = [p[0] for p in points]
            bers = [max(p[1], 1e-7) for p in points]  # Clamp for log scale
            name = config_name(config_id, is_nb)
            ax.semilogy(esn0s, bers, 'o-', label=name, color=cmap(i), markersize=3)

        ax.set_xlabel('Es/N0 (dB)')
        ax.set_ylabel('BER')
        ax.set_title(title)
        ax.set_ylim(1e-6, 1)
        ax.grid(True, alpha=0.3)
        ax.legend(fontsize=7, ncol=2, loc='upper right')
        fig.tight_layout()
        fig.savefig(output_path, dpi=150)
        plt.close(fig)
    except ImportError:
        print(f"  [CHART] matplotlib not available, skipping {output_path}")


# ============================================================
# INTEGRITY — end-to-end data correctness
# ============================================================

def run_integrity(args):
    """Run data integrity tests for specified configs."""
    import hashlib

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'integrity_{ts}')
    os.makedirs(output_dir, exist_ok=True)

    configs = parse_config_spec(getattr(args, 'configs', 'wb:102,wb:0,wb:8'),
                                default_nb=getattr(args, 'narrowband', False))

    print(f"\n{'='*70}")
    print(f"DATA INTEGRITY TEST — {len(configs)} configs")
    print(f"Output: {output_dir}")
    print(f"{'='*70}\n")

    results = []
    for config_id, is_nb in configs:
        name = config_name(config_id, is_nb)
        extra = build_config_extra_args(is_nb, args.signal_dbfs)
        test_size = getattr(args, 'test_size', None)
        # Smaller test for slow configs
        bps = config_max_bps(config_id, is_nb)
        if test_size is None:
            test_size = max(64, min(1024, int(bps * 0.3 * 60)))  # ~1 min of data

        print(f"  {name}: sending {test_size} bytes...", end='', flush=True)

        session = MercurySession(config_id, gearshift=False,
                                 mercury_path=args.mercury, extra_args=extra)
        try:
            session.start(timeout=30)
            if not session.wait_connected(timeout=120):
                print(f" CONNECT FAILED")
                results.append({'config': config_id, 'name': name,
                                'bandwidth': 'NB' if is_nb else 'WB',
                                'tx_bytes': 0, 'rx_bytes': 0,
                                'sha256_match': False, 'status': 'CONN_FAIL'})
                continue

            # Generate known pattern
            rng = random.Random(42)
            tx_data = bytes([rng.randint(0, 255) for _ in range(test_size)])
            tx_hash = hashlib.sha256(tx_data).hexdigest()

            # Send data via commander data port
            session._tx_sock.sendall(tx_data)

            # Wait for data to flow through
            wait_time = max(30, test_size * 8 / max(bps * 0.3, 1) + 30)
            time.sleep(min(wait_time, 300))

            # Read received data from RX
            rx_bytes = session._rx_bytes
            print(f" TX={test_size} RX={rx_bytes}", end='')

            # For integrity we check that bytes arrived (can't easily compare
            # exact bytes due to ARQ framing, but we verify count)
            status = 'PASS' if rx_bytes >= test_size * 0.9 else 'PARTIAL'
            if rx_bytes == 0:
                status = 'NO_DATA'

            print(f" [{status}]")
            results.append({'config': config_id, 'name': name,
                            'bandwidth': 'NB' if is_nb else 'WB',
                            'tx_bytes': test_size, 'rx_bytes': rx_bytes,
                            'sha256_match': status == 'PASS', 'status': status})
        except Exception as e:
            print(f" ERROR: {e}")
            results.append({'config': config_id, 'name': name,
                            'bandwidth': 'NB' if is_nb else 'WB',
                            'tx_bytes': 0, 'rx_bytes': 0,
                            'sha256_match': False, 'status': f'ERROR: {e}'})
        finally:
            log_path = os.path.join(output_dir, f'integrity_{name}.log')
            session.save_log(log_path)
            session.stop()

    # Write CSV
    csv_path = os.path.join(output_dir, 'integrity_results.csv')
    with open(csv_path, 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['config', 'config_name', 'bandwidth', 'tx_bytes', 'rx_bytes',
                     'sha256_match', 'status'])
        for r in results:
            w.writerow([r['config'], r['name'], r['bandwidth'], r['tx_bytes'],
                        r['rx_bytes'], r['sha256_match'], r['status']])
    print(f"\nCSV: {csv_path}")

    passed = sum(1 for r in results if r['status'] == 'PASS')
    print(f"Integrity: {passed}/{len(results)} PASS")
    return output_dir


# ============================================================
# NEGOTIATE — NB/WB auto-negotiation test
# ============================================================

def run_negotiate(args):
    """Test all 4 NB/WB negotiation combinations."""
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'negotiate_{ts}')
    os.makedirs(output_dir, exist_ok=True)

    iterations = getattr(args, 'iterations', 3)
    combos = [
        ('WB+WB', False, False, 'WB'),
        ('NB+NB', True,  True,  'NB'),
        ('WB+NB', False, True,  'NB'),
        ('NB+WB', True,  False, 'NB'),
    ]

    print(f"\n{'='*70}")
    print(f"NB/WB NEGOTIATION TEST — {len(combos)} combos × {iterations} iterations")
    print(f"Output: {output_dir}")
    print(f"{'='*70}\n")

    results = []
    for combo_name, cmd_nb, rsp_nb, expected_bw in combos:
        for iteration in range(iterations):
            print(f"  {combo_name} iter {iteration+1}/{iterations}...", end='', flush=True)
            os.system("taskkill /F /IM mercury.exe 2>nul >nul")
            time.sleep(2)

            # Build args — use -M for bandwidth mode, do NOT use -Q 0
            # (we want negotiation to happen)
            rsp_extra = ["-M", "nb"] if rsp_nb else ["-M", "auto"]
            cmd_extra = ["-M", "nb"] if cmd_nb else ["-M", "auto"]
            # Apply gain
            for extra, nb in [(rsp_extra, rsp_nb), (cmd_extra, cmd_nb)]:
                native = (NoiseInjector.DEFAULT_SIGNAL_DBFS_NB if nb
                          else NoiseInjector.DEFAULT_SIGNAL_DBFS)
                atten = args.signal_dbfs - native
                extra += ["-T", f"{atten:.1f}", "-G", f"{-atten:.1f}"]

            robust_flag = ["-R"]
            t0 = time.time()

            try:
                # Launch responder
                rsp_cmd = [args.mercury, "-m", "ARQ", "-s", "100",
                           *robust_flag, *rsp_extra,
                           "-p", str(RSP_PORT), "-i", VB_IN, "-o", VB_OUT,
                           "-x", "wasapi", "-n"]
                rsp = subprocess.Popen(rsp_cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.STDOUT)
                time.sleep(4)

                cmd_cmd = [args.mercury, "-m", "ARQ", "-s", "100",
                           *robust_flag, *cmd_extra,
                           "-p", str(CMD_PORT), "-i", VB_IN, "-o", VB_OUT,
                           "-x", "wasapi", "-n"]
                cmd = subprocess.Popen(cmd_cmd, stdout=subprocess.PIPE,
                                       stderr=subprocess.STDOUT)
                time.sleep(3)

                # TCP commands
                rsp_nb_tcp = ["BW500\r\n"] if rsp_nb else []
                rsp_ctrl = tcp_send_commands(RSP_PORT,
                    rsp_nb_tcp + ["MYCALL TESTB\r\n", "LISTEN ON\r\n"])
                time.sleep(1)

                cmd_nb_tcp = ["BW500\r\n"] if cmd_nb else []
                cmd_ctrl = tcp_send_commands(CMD_PORT,
                    cmd_nb_tcp + ["CONNECT TESTA TESTB\r\n"])

                # Wait for CONNECTED
                connected = False
                bandwidth = 'unknown'
                deadline = time.time() + 120
                buf = b''
                while time.time() < deadline:
                    try:
                        cmd_ctrl.settimeout(2)
                        data = cmd_ctrl.recv(4096)
                        if data:
                            buf += data
                            text = buf.decode('utf-8', errors='replace')
                            if 'CONNECTED' in text:
                                connected = True
                                # Parse bandwidth from CONNECTED response
                                for line in text.splitlines():
                                    if 'CONNECTED' in line:
                                        parts = line.split()
                                        if len(parts) >= 4:
                                            bw_hz = parts[-1]
                                            bandwidth = 'NB' if '500' in bw_hz or '468' in bw_hz else 'WB'
                                break
                    except socket.timeout:
                        if rsp.poll() is not None or cmd.poll() is not None:
                            break
                        continue

                elapsed = time.time() - t0
                status = 'PASS' if connected else 'FAIL'
                print(f" {status} (bw={bandwidth}, {elapsed:.1f}s)")

                results.append({
                    'combo': combo_name, 'iteration': iteration + 1,
                    'connected': connected, 'bandwidth': bandwidth,
                    'expected_bw': expected_bw, 'time_ms': int(elapsed * 1000),
                    'status': status
                })

                # Cleanup
                for s in [rsp_ctrl, cmd_ctrl]:
                    try: s.close()
                    except: pass
                for p in [rsp, cmd]:
                    try: p.terminate(); p.wait(3)
                    except: pass

            except Exception as e:
                print(f" ERROR: {e}")
                results.append({
                    'combo': combo_name, 'iteration': iteration + 1,
                    'connected': False, 'bandwidth': 'unknown',
                    'expected_bw': expected_bw, 'time_ms': 0,
                    'status': f'ERROR: {e}'
                })

    # Write CSV
    csv_path = os.path.join(output_dir, 'negotiate_results.csv')
    with open(csv_path, 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['combo', 'iteration', 'connected', 'bandwidth', 'expected_bw',
                     'time_ms', 'status'])
        for r in results:
            w.writerow([r['combo'], r['iteration'], r['connected'], r['bandwidth'],
                        r['expected_bw'], r['time_ms'], r['status']])
    print(f"\nCSV: {csv_path}")

    passed = sum(1 for r in results if r['status'] == 'PASS')
    print(f"Negotiation: {passed}/{len(results)} PASS")
    return output_dir


# ============================================================
# STABILITY — connection crash testing
# ============================================================

def run_stability(args):
    """Run repeated connect/disconnect cycles to test stability."""
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_dir = os.path.join(args.output_dir, f'stability_{ts}')
    os.makedirs(output_dir, exist_ok=True)

    count = getattr(args, 'count', 10)
    configs = parse_config_spec(getattr(args, 'configs', 'wb:100,wb:0'),
                                default_nb=getattr(args, 'narrowband', False))

    print(f"\n{'='*70}")
    print(f"CONNECTION STABILITY TEST — {len(configs)} configs × {count} iterations")
    print(f"Output: {output_dir}")
    print(f"{'='*70}\n")

    results = []
    for config_id, is_nb in configs:
        name = config_name(config_id, is_nb)
        extra = build_config_extra_args(is_nb, args.signal_dbfs)

        for iteration in range(count):
            print(f"  {name} iter {iteration+1}/{count}...", end='', flush=True)
            t0 = time.time()
            session = MercurySession(config_id, gearshift=False,
                                     mercury_path=args.mercury, extra_args=extra)
            connected = False
            crashed = False
            try:
                session.start(timeout=30)
                connected = session.wait_connected(timeout=60)
                if connected:
                    # Brief data exchange to verify link works
                    time.sleep(5)
                    alive = session.is_alive()
                    if not alive:
                        crashed = True
            except Exception as e:
                crashed = True
                print(f" ERROR: {e}", end='')

            elapsed = time.time() - t0
            status = 'PASS' if connected and not crashed else ('CRASH' if crashed else 'FAIL')
            print(f" {status} ({elapsed:.1f}s)")

            results.append({
                'config': config_id, 'name': name,
                'bandwidth': 'NB' if is_nb else 'WB',
                'iteration': iteration + 1,
                'connected': connected, 'crashed': crashed,
                'time_ms': int(elapsed * 1000), 'status': status
            })
            session.stop()
            time.sleep(2)

    # Write CSV
    csv_path = os.path.join(output_dir, 'stability_results.csv')
    with open(csv_path, 'w', newline='') as f:
        w = csv.writer(f)
        w.writerow(['config', 'config_name', 'bandwidth', 'iteration',
                     'connected', 'crashed', 'time_ms', 'status'])
        for r in results:
            w.writerow([r['config'], r['name'], r['bandwidth'], r['iteration'],
                        r['connected'], r['crashed'], r['time_ms'], r['status']])
    print(f"\nCSV: {csv_path}")

    passed = sum(1 for r in results if r['status'] == 'PASS')
    crashes = sum(1 for r in results if r['crashed'])
    print(f"Stability: {passed}/{len(results)} PASS, {crashes} crashes")
    return output_dir


# ============================================================
# FULL — run everything + generate report
# ============================================================

def run_full(args):
    """Run all benchmark phases and generate a collated report."""
    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    full_dir = os.path.join(args.output_dir, f'full_{ts}')
    os.makedirs(full_dir, exist_ok=True)

    max_workers = getattr(args, 'max_workers', 8) or 8
    progress_log = os.path.join(full_dir, 'progress.log')

    def log_progress(msg):
        line = f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}"
        print(line, flush=True)
        with open(progress_log, 'a') as f:
            f.write(line + '\n')

    log_progress(f"FULL BENCHMARK START — output: {full_dir}")
    log_progress(f"Mercury: {args.mercury}")
    log_progress(f"Max workers: {max_workers}")

    phase_results = {}
    phase_commands = {}

    # Phase 1: BER
    if getattr(args, 'skip_ber', False):
        log_progress("Phase 1: BER waterfall tests — SKIPPED (--skip-ber)")
        phase_results['ber'] = 'skipped'
    else:
        log_progress("Phase 1: BER waterfall tests")
        phase_commands['ber'] = (f"python mercury_benchmark.py ber "
                                 f"--configs all-wb,all-nb --max-workers {max_workers} "
                                 f"--output-dir {full_dir}/ber")
        try:
            ber_args = argparse.Namespace(
                mercury=args.mercury, output_dir=os.path.join(full_dir, 'ber'),
                configs='all-wb,all-nb', max_workers=max_workers,
                narrowband=False, signal_dbfs=args.signal_dbfs)
            phase_results['ber'] = run_ber(ber_args)
            log_progress("Phase 1: DONE")
        except Exception as e:
            log_progress(f"Phase 1: FAILED — {e}")
            phase_results['ber'] = None

    # Phase 2: WB sweep (parallel)
    log_progress("Phase 2: WB SNR sweep (parallel)")
    phase_commands['sweep_wb'] = (f"python mercury_benchmark.py sweep "
                                   f"--configs all-wb --parallel --max-workers {max_workers} "
                                   f"--snr-start 30 --snr-stop -20 --snr-step -3 "
                                   f"--measure-duration 120 --settle-time 15 "
                                   f"--signal-dbfs {args.signal_dbfs} "
                                   f"--output-dir {full_dir}/sweep_wb")
    try:
        sweep_wb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'sweep_wb'),
            configs='all-wb', parallel=True, max_workers=max_workers,
            snr_start=30, snr_stop=-20, snr_step=-3,
            measure_duration=120, auto_duration=True, no_auto_duration=False,
            settle_time=15, waterfall_threshold=None, no_waterfall=False,
            narrowband=False, signal_dbfs=args.signal_dbfs, timeout=600)
        run_parallel_sweep(sweep_wb_args)
        phase_results['sweep_wb'] = os.path.join(full_dir, 'sweep_wb')
        log_progress("Phase 2: DONE")
    except Exception as e:
        log_progress(f"Phase 2: FAILED — {e}")
        phase_results['sweep_wb'] = None

    # Phase 3: NB sweep (parallel)
    log_progress("Phase 3: NB SNR sweep (parallel)")
    phase_commands['sweep_nb'] = (f"python mercury_benchmark.py sweep "
                                   f"--configs all-nb --parallel --max-workers {max_workers} "
                                   f"--snr-start 30 --snr-stop -20 --snr-step -3 "
                                   f"--measure-duration 120 --settle-time 15 "
                                   f"--signal-dbfs {args.signal_dbfs} "
                                   f"--output-dir {full_dir}/sweep_nb")
    try:
        sweep_nb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'sweep_nb'),
            configs='all-nb', parallel=True, max_workers=max_workers,
            snr_start=30, snr_stop=-20, snr_step=-3,
            measure_duration=120, auto_duration=True, no_auto_duration=False,
            settle_time=15, waterfall_threshold=None, no_waterfall=False,
            narrowband=True, signal_dbfs=args.signal_dbfs, timeout=600)
        run_parallel_sweep(sweep_nb_args)
        phase_results['sweep_nb'] = os.path.join(full_dir, 'sweep_nb')
        log_progress("Phase 3: DONE")
    except Exception as e:
        log_progress(f"Phase 3: FAILED — {e}")
        phase_results['sweep_nb'] = None

    # Phase 4: WB stress
    log_progress("Phase 4: WB stress test")
    phase_commands['stress_wb'] = (f"python mercury_benchmark.py stress "
                                    f"--num-bursts 10 --start-config 100 "
                                    f"--signal-dbfs {args.signal_dbfs} "
                                    f"--output-dir {full_dir}/stress_wb")
    try:
        stress_wb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'stress_wb'),
            num_bursts=10, min_dur=60, max_dur=180,
            snr_low=-3, snr_high=15, start_config=100, seed=None,
            narrowband=False, signal_dbfs=args.signal_dbfs, timeout=600)
        run_stress(stress_wb_args)
        phase_results['stress_wb'] = os.path.join(full_dir, 'stress_wb')
        log_progress("Phase 4: DONE")
    except Exception as e:
        log_progress(f"Phase 4: FAILED — {e}")
        phase_results['stress_wb'] = None

    # Phase 5: NB stress
    log_progress("Phase 5: NB stress test")
    phase_commands['stress_nb'] = (f"python mercury_benchmark.py stress "
                                    f"--num-bursts 10 --start-config 100 -N "
                                    f"--signal-dbfs {args.signal_dbfs} "
                                    f"--output-dir {full_dir}/stress_nb")
    try:
        stress_nb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'stress_nb'),
            num_bursts=10, min_dur=60, max_dur=180,
            snr_low=-3, snr_high=15, start_config=100, seed=None,
            narrowband=True, signal_dbfs=args.signal_dbfs, timeout=600)
        run_stress(stress_nb_args)
        phase_results['stress_nb'] = os.path.join(full_dir, 'stress_nb')
        log_progress("Phase 5: DONE")
    except Exception as e:
        log_progress(f"Phase 5: FAILED — {e}")
        phase_results['stress_nb'] = None

    # Phase 6: WB adaptive
    log_progress("Phase 6: WB adaptive gearshift")
    phase_commands['adaptive_wb'] = (f"python mercury_benchmark.py adaptive "
                                      f"--snr-start 30 --snr-stop -15 --snr-step -3 "
                                      f"--measure-duration 120 --settle-time 15 "
                                      f"--signal-dbfs {args.signal_dbfs} "
                                      f"--output-dir {full_dir}/adaptive_wb")
    try:
        adaptive_wb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'adaptive_wb'),
            snr_start=30, snr_stop=-15, snr_step=-3,
            measure_duration=120, settle_time=15, round_trip=True,
            narrowband=False, signal_dbfs=args.signal_dbfs, timeout=600)
        run_adaptive(adaptive_wb_args)
        phase_results['adaptive_wb'] = os.path.join(full_dir, 'adaptive_wb')
        log_progress("Phase 6: DONE")
    except Exception as e:
        log_progress(f"Phase 6: FAILED — {e}")
        phase_results['adaptive_wb'] = None

    # Phase 7: NB adaptive
    log_progress("Phase 7: NB adaptive gearshift")
    phase_commands['adaptive_nb'] = (f"python mercury_benchmark.py adaptive "
                                      f"--snr-start 30 --snr-stop -15 --snr-step -3 "
                                      f"--measure-duration 120 --settle-time 15 "
                                      f"-N --signal-dbfs {args.signal_dbfs} "
                                      f"--output-dir {full_dir}/adaptive_nb")
    try:
        adaptive_nb_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'adaptive_nb'),
            snr_start=30, snr_stop=-15, snr_step=-3,
            measure_duration=120, settle_time=15, round_trip=True,
            narrowband=True, signal_dbfs=args.signal_dbfs, timeout=600)
        run_adaptive(adaptive_nb_args)
        phase_results['adaptive_nb'] = os.path.join(full_dir, 'adaptive_nb')
        log_progress("Phase 7: DONE")
    except Exception as e:
        log_progress(f"Phase 7: FAILED — {e}")
        phase_results['adaptive_nb'] = None

    # Phase 8: Integrity
    log_progress("Phase 8: Data integrity tests")
    phase_commands['integrity'] = (f"python mercury_benchmark.py integrity "
                                    f"--configs wb:100,wb:102,wb:0,wb:8,wb:15 "
                                    f"--signal-dbfs {args.signal_dbfs} "
                                    f"--output-dir {full_dir}/integrity")
    try:
        integrity_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'integrity'),
            configs='wb:100,wb:102,wb:0,wb:8,wb:15',
            narrowband=False, signal_dbfs=args.signal_dbfs,
            test_size=None, timeout=600)
        phase_results['integrity'] = run_integrity(integrity_args)
        log_progress("Phase 8: DONE")
    except Exception as e:
        log_progress(f"Phase 8: FAILED — {e}")
        phase_results['integrity'] = None

    # Phase 9: Negotiation
    log_progress("Phase 9: NB/WB negotiation tests")
    phase_commands['negotiate'] = (f"python mercury_benchmark.py negotiate "
                                    f"--iterations 3 "
                                    f"--signal-dbfs {args.signal_dbfs} "
                                    f"--output-dir {full_dir}/negotiate")
    try:
        negotiate_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'negotiate'),
            iterations=3, narrowband=False, signal_dbfs=args.signal_dbfs, timeout=600)
        phase_results['negotiate'] = run_negotiate(negotiate_args)
        log_progress("Phase 9: DONE")
    except Exception as e:
        log_progress(f"Phase 9: FAILED — {e}")
        phase_results['negotiate'] = None

    # Phase 10: Stability
    log_progress("Phase 10: Connection stability tests")
    phase_commands['stability'] = (f"python mercury_benchmark.py stability "
                                    f"--count 10 --configs wb:100,wb:0 "
                                    f"--signal-dbfs {args.signal_dbfs} "
                                    f"--output-dir {full_dir}/stability")
    try:
        stability_args = argparse.Namespace(
            mercury=args.mercury, output_dir=os.path.join(full_dir, 'stability'),
            count=10, configs='wb:100,wb:0',
            narrowband=False, signal_dbfs=args.signal_dbfs, timeout=600)
        phase_results['stability'] = run_stability(stability_args)
        log_progress("Phase 10: DONE")
    except Exception as e:
        log_progress(f"Phase 10: FAILED — {e}")
        phase_results['stability'] = None

    # Phase 11: Generate report
    log_progress("Phase 11: Generating report")
    try:
        generate_report(full_dir, phase_results, phase_commands, args)
        log_progress("Phase 11: DONE")
    except Exception as e:
        log_progress(f"Phase 11: FAILED — {e}")

    log_progress("FULL BENCHMARK COMPLETE")
    print(f"\nResults: {full_dir}")
    print(f"Report: {os.path.join(full_dir, 'RESULTS_SUMMARY.md')}")


def generate_report(full_dir, phase_results, phase_commands, args):
    """Generate collated Markdown report from all phase results."""
    report_path = os.path.join(full_dir, 'RESULTS_SUMMARY.md')
    ts = datetime.now().strftime('%Y-%m-%d %H:%M')

    # Gather stats from CSVs
    def read_csv_rows(subdir_pattern, csv_name):
        """Find and read CSV from a phase output directory."""
        for name, path in phase_results.items():
            if path and name.startswith(subdir_pattern):
                csv_path = os.path.join(path, csv_name)
                if os.path.isfile(csv_path):
                    with open(csv_path) as f:
                        return list(csv.DictReader(f))
        # Try direct path
        for root, dirs, files in os.walk(full_dir):
            if csv_name in files:
                fpath = os.path.join(root, csv_name)
                if subdir_pattern in fpath:
                    with open(fpath) as f:
                        return list(csv.DictReader(f))
        return []

    # Find chart files
    def find_charts(subdir):
        """Find PNG files in a phase subdirectory."""
        charts = []
        search_dir = os.path.join(full_dir, subdir)
        if os.path.isdir(search_dir):
            for f in sorted(os.listdir(search_dir)):
                if f.endswith('.png'):
                    charts.append(os.path.join(subdir, f))
        # Also check nested timestamped dirs
        for root, dirs, files in os.walk(search_dir):
            for f in sorted(files):
                if f.endswith('.png'):
                    rel = os.path.relpath(os.path.join(root, f), full_dir)
                    if rel not in charts:
                        charts.append(rel)
        return charts

    lines = []
    lines.append(f"# Mercury Modem — Benchmark Results")
    lines.append(f"_Generated {ts} — Mercury v0.3.1-dev1_\n")

    # Executive summary
    lines.append("## Executive Summary\n")
    integrity_rows = read_csv_rows('integrity', 'integrity_results.csv')
    negotiate_rows = read_csv_rows('negotiate', 'negotiate_results.csv')
    stability_rows = read_csv_rows('stability', 'stability_results.csv')

    integrity_pass = sum(1 for r in integrity_rows if r.get('status') == 'PASS')
    negotiate_pass = sum(1 for r in negotiate_rows if r.get('status') == 'PASS')
    stability_pass = sum(1 for r in stability_rows if r.get('status') == 'PASS')
    stability_crash = sum(1 for r in stability_rows if r.get('crashed') == 'True')

    lines.append(f"- Data integrity: **{integrity_pass}/{len(integrity_rows)}** PASS")
    lines.append(f"- NB/WB negotiation: **{negotiate_pass}/{len(negotiate_rows)}** PASS")
    lines.append(f"- Connection stability: **{stability_pass}/{len(stability_rows)}** PASS, "
                 f"**{stability_crash}** crashes")
    lines.append("")

    # Phase results
    phase_info = [
        ('ber', '1. BER Waterfall Performance', 'ber'),
        ('sweep_wb', '2. WB Throughput vs SNR', 'sweep_wb'),
        ('sweep_nb', '3. NB Throughput vs SNR', 'sweep_nb'),
        ('stress_wb', '4. WB Stress Test', 'stress_wb'),
        ('stress_nb', '5. NB Stress Test', 'stress_nb'),
        ('adaptive_wb', '6. WB Adaptive Gearshift', 'adaptive_wb'),
        ('adaptive_nb', '7. NB Adaptive Gearshift', 'adaptive_nb'),
    ]

    for phase_key, title, subdir in phase_info:
        lines.append(f"## {title}\n")
        if phase_results.get(phase_key):
            charts = find_charts(subdir)
            for chart in charts:
                lines.append(f"![{title}]({chart})\n")
            if not charts:
                lines.append("_(no charts generated)_\n")
        else:
            lines.append("_(phase failed or skipped)_\n")
        # Replication command
        if phase_key in phase_commands:
            lines.append(f"**Replication command:**")
            lines.append(f"```")
            lines.append(phase_commands[phase_key])
            lines.append(f"```\n")

    # Integrity table
    lines.append("## 8. Data Integrity\n")
    if integrity_rows:
        lines.append("| Config | BW | TX bytes | RX bytes | Status |")
        lines.append("|--------|-----|----------|----------|--------|")
        for r in integrity_rows:
            lines.append(f"| {r.get('config_name','')} | {r.get('bandwidth','')} | "
                         f"{r.get('tx_bytes','')} | {r.get('rx_bytes','')} | "
                         f"{r.get('status','')} |")
        lines.append("")
    if 'integrity' in phase_commands:
        lines.append(f"**Replication command:**")
        lines.append(f"```")
        lines.append(phase_commands['integrity'])
        lines.append(f"```\n")

    # Negotiation table
    lines.append("## 9. NB/WB Negotiation\n")
    if negotiate_rows:
        lines.append("| Combo | Iter | Connected | BW | Expected | Time | Status |")
        lines.append("|-------|------|-----------|----|----------|------|--------|")
        for r in negotiate_rows:
            lines.append(f"| {r.get('combo','')} | {r.get('iteration','')} | "
                         f"{r.get('connected','')} | {r.get('bandwidth','')} | "
                         f"{r.get('expected_bw','')} | {r.get('time_ms','')}ms | "
                         f"{r.get('status','')} |")
        lines.append("")
    if 'negotiate' in phase_commands:
        lines.append(f"**Replication command:**")
        lines.append(f"```")
        lines.append(phase_commands['negotiate'])
        lines.append(f"```\n")

    # Stability table
    lines.append("## 10. Connection Stability\n")
    if stability_rows:
        lines.append("| Config | BW | Iter | Connected | Crashed | Time | Status |")
        lines.append("|--------|-----|------|-----------|---------|------|--------|")
        for r in stability_rows:
            lines.append(f"| {r.get('config_name','')} | {r.get('bandwidth','')} | "
                         f"{r.get('iteration','')} | {r.get('connected','')} | "
                         f"{r.get('crashed','')} | {r.get('time_ms','')}ms | "
                         f"{r.get('status','')} |")
        lines.append("")
    if 'stability' in phase_commands:
        lines.append(f"**Replication command:**")
        lines.append(f"```")
        lines.append(phase_commands['stability'])
        lines.append(f"```\n")

    # Test environment
    lines.append("## Test Environment\n")
    lines.append(f"- **Platform**: {sys.platform}")
    lines.append(f"- **Mercury**: {args.mercury}")
    lines.append(f"- **Signal level**: {args.signal_dbfs} dBFS")
    lines.append(f"- **Audio**: VB-Audio Virtual Cable (WASAPI)")
    lines.append(f"- **Parallel workers**: {getattr(args, 'max_workers', 8) or 8}")
    lines.append(f"- **Date**: {ts}")
    lines.append("")

    # Full replication
    lines.append("## Full Replication\n")
    lines.append("To reproduce all results in one run:")
    lines.append("```")
    lines.append(f"python mercury_benchmark.py full --parallel {getattr(args, 'max_workers', 8) or 8} "
                 f"--signal-dbfs {args.signal_dbfs} --output-dir ./benchmark_results")
    lines.append("```\n")

    with open(report_path, 'w') as f:
        f.write('\n'.join(lines))
    print(f"Report: {report_path}")


# ============================================================
# VARA-SWEEP — throughput vs SNR for VARA HF
# ============================================================

def run_vara_sweep(args):
    """SNR sweep using VARA HF (connects to user-started instances)."""
    snr_levels_down = []
    snr = args.snr_start
    while snr >= args.snr_stop:
        snr_levels_down.append(snr)
        snr += args.snr_step

    snr_levels = list(snr_levels_down)
    if args.round_trip:
        snr_levels += list(reversed(snr_levels_down[:-1]))

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    os.makedirs(args.output_dir, exist_ok=True)
    csv_path = os.path.join(args.output_dir, f'vara_sweep_{ts}.csv')
    chart_path = os.path.join(args.output_dir, f'vara_sweep_{ts}.png')
    log_path = os.path.join(args.output_dir, f'vara_sweep_{ts}.log')

    bw = getattr(args, 'bandwidth', '2300')

    print(f"{'='*70}")
    print(f"VARA HF Benchmark — Adaptive SNR Sweep (BW={bw})")
    print(f"{'='*70}")
    print(f"Responder port: {args.rsp_port}")
    print(f"Commander port: {args.cmd_port}")
    print(f"SNR levels: {snr_levels} dB")
    print(f"Measurement duration: {args.measure_duration}s per point")
    print(f"Signal ref: {args.signal_dbfs} dBFS (set via Windows mixer)")
    print(f"Round trip: {'yes' if args.round_trip else 'no'}")
    est_time = len(snr_levels) * (args.measure_duration + args.settle_time) + 60
    print(f"Estimated time: {est_time/60:.0f} min")
    print()

    device_idx = find_wasapi_cable_output()
    noise = NoiseInjector(device_idx, snr_db=30, sample_rate=SAMPLE_RATE,
                           signal_dbfs=args.signal_dbfs)
    noise.start()

    session = VaraSession(rsp_port=args.rsp_port, cmd_port=args.cmd_port,
                           bandwidth=bw)
    results = []
    fieldnames = ['snr_db', 'direction', 'rx_bytes', 'duration_s',
                  'throughput_bps', 'bytes_per_min']

    try:
        print("[VARA] Connecting to VARA instances...")
        session.start(timeout=30)

        if not session.wait_connected(timeout=args.timeout):
            print("[ERROR] VARA connection failed — are both instances running?")
            return
        print("[OK] VARA CONNECTED")

        # Let VARA settle and start gearshifting up
        print("[SETTLE] Waiting 30s for VARA to stabilize and gear up...")
        time.sleep(30)
        print(f"[OK] Data flowing. RX={session.get_rx_bytes()} bytes")
        print()

        half = len(snr_levels_down)

        for snr_idx, snr_db in enumerate(snr_levels):
            direction = "DOWN" if snr_idx < half else "UP"

            if not session.is_alive():
                print(f"[DEAD] VARA connection lost at SNR={snr_db}dB")
                for remaining_snr in snr_levels[snr_idx:]:
                    results.append({
                        'snr_db': remaining_snr, 'direction': direction,
                        'rx_bytes': 0, 'duration_s': 0, 'throughput_bps': 0,
                        'bytes_per_min': 0,
                    })
                break

            noise.set_snr(snr_db)
            noise.noise_on()

            result = session.measure_throughput(args.measure_duration)

            noise.noise_off()

            row = {
                'snr_db': snr_db, 'direction': direction,
                'rx_bytes': result['rx_bytes'],
                'duration_s': round(result['duration_s'], 1),
                'throughput_bps': round(result['throughput_bps'], 1),
                'bytes_per_min': round(result['bytes_per_min'], 1),
            }
            results.append(row)

            print(f"  [{snr_idx+1}/{len(snr_levels)}] {direction} SNR={snr_db:+.0f}dB: "
                  f"{result['bytes_per_min']:.0f} B/min ({result['rx_bytes']}B in "
                  f"{result['duration_s']:.0f}s)")

            if snr_idx < len(snr_levels) - 1:
                time.sleep(args.settle_time)

        # Write CSV
        with open(csv_path, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(results)
        print(f"\n[CSV] Saved to {csv_path}")

        # Generate chart
        try:
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt

            down_rows = [r for r in results if r['direction'] == 'DOWN']
            up_rows = [r for r in results if r['direction'] == 'UP']

            fig, ax = plt.subplots(figsize=(14, 8))
            if down_rows:
                ax.plot([r['snr_db'] for r in down_rows],
                        [r['bytes_per_min'] for r in down_rows],
                        'bo-', linewidth=2, markersize=5, label='SNR decreasing')
            if up_rows:
                ax.plot([r['snr_db'] for r in up_rows],
                        [r['bytes_per_min'] for r in up_rows],
                        'rs-', linewidth=2, markersize=5, label='SNR increasing')

            ax.set_xlabel('SNR (dB in 4 kHz)', fontsize=12)
            ax.set_ylabel('Throughput (bytes/min)', fontsize=12)
            ax.set_title(f'VARA HF — Adaptive Throughput vs SNR (BW={bw})', fontsize=14)
            ax.legend(fontsize=10)
            ax.grid(True, alpha=0.3)
            if any(r['bytes_per_min'] > 0 for r in results):
                ax.set_yscale('log')
            plt.tight_layout()
            plt.savefig(chart_path, dpi=150)
            print(f"[CHART] Saved to {chart_path}")
            plt.close()
        except ImportError:
            print("[CHART] matplotlib not available")

        print(f"\n{'='*70}")
        print(f"VARA SWEEP COMPLETE — {len(results)} data points")
        print(f"{'='*70}")

    finally:
        noise.stop()
        session.save_log(log_path)
        session.stop()


# ============================================================
# COMPARE — overlay Mercury and VARA on same chart
# ============================================================

def run_compare(args):
    """Generate comparison chart overlaying Mercury and VARA sweep results."""
    try:
        import matplotlib
        matplotlib.use('Agg')
        import matplotlib.pyplot as plt
    except ImportError:
        print("[ERROR] matplotlib required for comparison charts")
        return

    output_path = getattr(args, 'output', None) or os.path.join(
        args.output_dir, f'comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
    os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)

    fig, ax = plt.subplots(figsize=(16, 10))

    # Plot Mercury CSVs (may be multiple — one per config from sweep)
    mercury_csvs = getattr(args, 'mercury_csv', []) or []
    if isinstance(mercury_csvs, str):
        mercury_csvs = [mercury_csvs]
    # Expand globs
    import glob as globmod
    expanded = []
    for pattern in mercury_csvs:
        matches = globmod.glob(pattern)
        expanded.extend(matches if matches else [pattern])
    mercury_csvs = expanded

    mercury_configs = {}
    for csv_file in mercury_csvs:
        if not os.path.isfile(csv_file):
            print(f"  [WARN] Mercury CSV not found: {csv_file}")
            continue
        with open(csv_file) as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Sweep CSVs have: config, config_name, snr_db, bytes_per_min
                # Adaptive CSVs have: snr_db, direction, bytes_per_min
                cfg_name = row.get('config_name', 'Mercury')
                snr = float(row['snr_db'])
                bpm = float(row['bytes_per_min'])
                direction = row.get('direction', 'DOWN')
                if direction != 'DOWN':
                    continue  # Only plot the down sweep for clarity
                if cfg_name not in mercury_configs:
                    mercury_configs[cfg_name] = {'snr': [], 'bpm': []}
                mercury_configs[cfg_name]['snr'].append(snr)
                mercury_configs[cfg_name]['bpm'].append(bpm)

    # Plot Mercury lines
    from matplotlib.cm import get_cmap
    if mercury_configs:
        cmap = get_cmap('Blues', max(len(mercury_configs) + 2, 4))
        for i, (name, data) in enumerate(sorted(mercury_configs.items())):
            ax.plot(data['snr'], data['bpm'], 'o-', color=cmap(i + 2),
                    linewidth=1.5, markersize=4, label=f'Mercury {name}', alpha=0.8)

    # Plot VARA CSVs
    vara_csvs = getattr(args, 'vara_csv', []) or []
    if isinstance(vara_csvs, str):
        vara_csvs = [vara_csvs]
    expanded = []
    for pattern in vara_csvs:
        matches = globmod.glob(pattern)
        expanded.extend(matches if matches else [pattern])
    vara_csvs = expanded

    vara_idx = 0
    vara_colors = ['#FF4444', '#FF8800', '#FF00AA']
    for csv_file in vara_csvs:
        if not os.path.isfile(csv_file):
            print(f"  [WARN] VARA CSV not found: {csv_file}")
            continue
        snrs, bpms = [], []
        with open(csv_file) as f:
            reader = csv.DictReader(f)
            for row in reader:
                direction = row.get('direction', 'DOWN')
                if direction != 'DOWN':
                    continue
                snrs.append(float(row['snr_db']))
                bpms.append(float(row['bytes_per_min']))
        color = vara_colors[vara_idx % len(vara_colors)]
        ax.plot(snrs, bpms, 's-', color=color, linewidth=2.5, markersize=6,
                label=f'VARA HF', alpha=0.9)
        vara_idx += 1

    ax.set_xlabel('SNR (dB in 4 kHz)', fontsize=12)
    ax.set_ylabel('Throughput (bytes/min)', fontsize=12)
    ax.set_title('Mercury vs VARA HF — Throughput vs SNR', fontsize=14)
    ax.legend(fontsize=8, ncol=2, loc='upper left')
    ax.grid(True, alpha=0.3)
    all_bpm = []
    for data in mercury_configs.values():
        all_bpm.extend(data['bpm'])
    if any(b > 0 for b in all_bpm):
        ax.set_yscale('log')
    plt.tight_layout()
    plt.savefig(output_path, dpi=150)
    print(f"[CHART] Comparison saved to {output_path}")
    plt.close()


# ============================================================
# CLI
# ============================================================

def main():
    parser = argparse.ArgumentParser(
        description='Mercury Modem Benchmark & Stress Test Suite',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument('--mercury', default=MERCURY_DEFAULT,
                        help=f'Path to mercury.exe (default: {MERCURY_DEFAULT})')
    parser.add_argument('--output-dir', default='./benchmark_results',
                        help='Output directory for CSV/charts/logs')
    parser.add_argument('--timeout', type=float, default=600,
                        help='Per-scenario timeout in seconds')
    parser.add_argument('--signal-dbfs', type=float, default=-30.0,
                        help='Cable signal level in dBFS (default: -30.0). '
                             'TX is attenuated and RX boosted to match. '
                             'Noise is calibrated to this level for correct SNR.')
    parser.add_argument('--tx-attenuate', action='store_true',
                        help='(Legacy) Attenuate Mercury TX and boost RX. '
                             'Now always enabled — TX/RX gain is auto-computed per config.')
    parser.add_argument('--narrowband', '-N', action='store_true',
                        help='Default NB mode for configs without nb:/wb: prefix. '
                             'Not needed when using prefixed config specs like nb:100,wb:0.')

    sub = parser.add_subparsers(dest='command', help='Sub-command')

    # sweep
    p_sweep = sub.add_parser('sweep', help='Fixed-config SNR sweep',
                             epilog='Config spec: nb:100,wb:101,0 or presets: '
                                    'nb-mfsk, wb-mfsk, nb-ofdm, wb-ofdm, all-nb, all-wb')
    p_sweep.add_argument('--configs', default='nb-mfsk,wb-mfsk,nb-ofdm',
                         help='Config specs: nb:100,wb:0 or presets (nb-mfsk,wb-mfsk,nb-ofdm,...)')
    p_sweep.add_argument('--snr-start', type=float, default=30,
                         help='Starting SNR (dB)')
    p_sweep.add_argument('--snr-stop', type=float, default=-20,
                         help='Ending SNR (dB)')
    p_sweep.add_argument('--snr-step', type=float, default=-3,
                         help='SNR step (negative)')
    p_sweep.add_argument('--measure-duration', type=float, default=120,
                         help='Base measurement time per SNR point (seconds)')
    p_sweep.add_argument('--auto-duration', action='store_true', default=True,
                         help='Auto-extend measurement for slow modes (default: on)')
    p_sweep.add_argument('--no-auto-duration', action='store_true',
                         help='Disable auto-duration extension')
    p_sweep.add_argument('--settle-time', type=float, default=15,
                         help='Recovery time between SNR points (seconds)')
    p_sweep.add_argument('--waterfall-threshold', type=int, default=None,
                         help='Consecutive zero-throughput points before skipping '
                              '(default: 4 for ROBUST, 2 for OFDM). Use 0 to disable.')
    p_sweep.add_argument('--no-waterfall', action='store_true',
                         help='Disable waterfall detection entirely')
    p_sweep.add_argument('--parallel', action='store_true',
                         help='Enable multi-channel parallel mode (uses all discovered VB-Cable A/B channels)')
    p_sweep.add_argument('--max-workers', type=int, default=None,
                         help='Limit number of parallel workers (default: all available channels)')

    # stress
    p_stress = sub.add_parser('stress', help='Random noise stress test')
    p_stress.add_argument('--num-bursts', type=int, default=10,
                          help='Number of noise bursts')
    p_stress.add_argument('--min-dur', type=float, default=60,
                          help='Minimum burst duration (seconds)')
    p_stress.add_argument('--max-dur', type=float, default=180,
                          help='Maximum burst duration (seconds)')
    p_stress.add_argument('--snr-low', type=float, default=-3,
                          help='Lowest burst SNR (dB)')
    p_stress.add_argument('--snr-high', type=float, default=15,
                          help='Highest burst SNR (dB)')
    p_stress.add_argument('--start-config', type=int, default=100,
                          help='Starting config (default: ROBUST_0)')
    p_stress.add_argument('--seed', type=int, default=None,
                          help='Random seed for reproducibility')

    # adaptive
    p_adaptive = sub.add_parser('adaptive', help='Gearshift SNR sweep')
    p_adaptive.add_argument('--snr-start', type=float, default=30,
                            help='Starting SNR (dB)')
    p_adaptive.add_argument('--snr-stop', type=float, default=-5,
                            help='Ending SNR (dB)')
    p_adaptive.add_argument('--snr-step', type=float, default=-3,
                            help='SNR step (negative)')
    p_adaptive.add_argument('--measure-duration', type=float, default=120,
                            help='Measurement time per SNR point (seconds)')
    p_adaptive.add_argument('--settle-time', type=float, default=15,
                            help='Recovery time between SNR points (seconds)')
    p_adaptive.add_argument('--no-round-trip', action='store_true',
                            help='Skip the sweep back up')

    # ber
    p_ber = sub.add_parser('ber', help='BER waterfall (PLOT_PASSBAND loopback)')
    p_ber.add_argument('--configs', default='all-wb,all-nb',
                       help='Config specs (default: all-wb,all-nb)')
    p_ber.add_argument('--max-workers', type=int, default=8,
                       help='Max parallel BER processes (default: 8)')

    # integrity
    p_integrity = sub.add_parser('integrity', help='End-to-end data correctness')
    p_integrity.add_argument('--configs', default='wb:100,wb:102,wb:0,wb:8,wb:15',
                             help='Config specs to test (default: wb:100,wb:102,wb:0,wb:8,wb:15)')
    p_integrity.add_argument('--test-size', type=int, default=None,
                             help='Bytes to send per config (default: auto based on throughput)')

    # negotiate
    p_negotiate = sub.add_parser('negotiate', help='NB/WB auto-negotiation test')
    p_negotiate.add_argument('--iterations', type=int, default=3,
                             help='Iterations per NB/WB combo (default: 3)')

    # stability
    p_stability = sub.add_parser('stability', help='Connection crash/stability test')
    p_stability.add_argument('--count', type=int, default=10,
                             help='Connect/disconnect cycles per config (default: 10)')
    p_stability.add_argument('--configs', default='wb:100,wb:0',
                             help='Config specs to test (default: wb:100,wb:0)')

    # full
    p_full = sub.add_parser('full', help='Run all benchmarks + generate report')
    p_full.add_argument('--max-workers', type=int, default=8,
                        help='Max parallel workers (default: 8)')
    p_full.add_argument('--skip-ber', action='store_true',
                        help='Skip BER waterfall tests (Phase 1)')

    # vara-sweep
    p_vara = sub.add_parser('vara-sweep',
                             help='VARA HF throughput sweep (connect to running instances)')
    p_vara.add_argument('--rsp-port', type=int, default=8300,
                        help='VARA responder TCP control port (default: 8300)')
    p_vara.add_argument('--cmd-port', type=int, default=8310,
                        help='VARA commander TCP control port (default: 8310)')
    p_vara.add_argument('--bandwidth', default='2300',
                        help='VARA bandwidth: 2300 (WB) or 500 (NB) (default: 2300)')
    p_vara.add_argument('--snr-start', type=float, default=30,
                        help='Starting SNR (dB)')
    p_vara.add_argument('--snr-stop', type=float, default=-20,
                        help='Ending SNR (dB)')
    p_vara.add_argument('--snr-step', type=float, default=-3,
                        help='SNR step (negative)')
    p_vara.add_argument('--measure-duration', type=float, default=120,
                        help='Measurement time per SNR point (seconds)')
    p_vara.add_argument('--settle-time', type=float, default=15,
                        help='Recovery time between SNR points (seconds)')
    p_vara.add_argument('--no-round-trip', action='store_true',
                        help='Skip the sweep back up')

    # compare
    p_compare = sub.add_parser('compare',
                                help='Generate Mercury vs VARA comparison chart')
    p_compare.add_argument('--mercury-csv', nargs='+', required=True,
                           help='Mercury sweep CSV file(s) (supports glob patterns)')
    p_compare.add_argument('--vara-csv', nargs='+', required=True,
                           help='VARA sweep CSV file(s) (supports glob patterns)')
    p_compare.add_argument('--output', default=None,
                           help='Output PNG path (default: auto-generated)')

    args = parser.parse_args()

    if not args.command:
        parser.print_help()
        sys.exit(1)

    # Warn if mercury.ini has mismatched TX/RX gain (common misconfiguration for loopback)
    ini_path = os.path.join(os.environ.get('APPDATA', ''), 'Mercury', 'mercury.ini')
    if os.path.isfile(ini_path):
        try:
            import configparser
            ini = configparser.ConfigParser()
            ini.read(ini_path)
            tx_db = float(ini.get('GUI', 'TxGainDb', fallback='0'))
            rx_db = float(ini.get('GUI', 'RxGainDb', fallback='0'))
            if abs(tx_db - rx_db) > 1.0:
                print(f"WARNING: mercury.ini has TxGainDb={tx_db} RxGainDb={rx_db} "
                      f"(difference: {abs(tx_db - rx_db):.1f} dB)")
                print(f"  The benchmark uses -T/-G flags to set test levels, but the INI")
                print(f"  values are still applied by the GUI. For loopback testing,")
                print(f"  ensure TxGainDb and RxGainDb are both 0 in the INI, or use")
                print(f"  -n (nogui) mode where INI gains are not applied.")
                print()
        except Exception:
            pass  # Don't block benchmark on INI parse failure

    # Propagate common args
    args.output_dir = os.path.abspath(args.output_dir)

    # Resolve auto-duration for sweep
    if args.command == 'sweep' and getattr(args, 'no_auto_duration', False):
        args.auto_duration = False

    if args.command == 'sweep':
        if getattr(args, 'parallel', False):
            run_parallel_sweep(args)
        else:
            run_sweep(args)
    elif args.command == 'stress':
        run_stress(args)
    elif args.command == 'adaptive':
        args.round_trip = not args.no_round_trip
        run_adaptive(args)
    elif args.command == 'ber':
        run_ber(args)
    elif args.command == 'integrity':
        run_integrity(args)
    elif args.command == 'negotiate':
        run_negotiate(args)
    elif args.command == 'stability':
        run_stability(args)
    elif args.command == 'full':
        run_full(args)
    elif args.command == 'vara-sweep':
        args.round_trip = not args.no_round_trip
        run_vara_sweep(args)
    elif args.command == 'compare':
        run_compare(args)


if __name__ == "__main__":
    main()
